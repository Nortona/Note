![[Pasted image 20230731212952.png]]



# 比特率
显示一段视频每秒所需的比特数量就是它的**比特率**。
> 比特率 = 宽 * 高 * 比特深度 * 帧每秒

例如，一段每秒30帧，每像素24比特，分辨率是480x240的视频，如果我们不做任何压缩，它将需要 **82,944,000 比特每秒**或 82.944 Mbps (30x480x240x24)。


# 颜色编码方式

## RGB
24位 8+8+8

## YUV
YUV数据由Y、U、V三个分量组成，现在通常说的YUV指的是**YCbCr**。
- **Y**：表示亮度（Luminance、Luma），占8bit（1字节）
- **Cb**、**Cr**：表示色度（Chrominance、Chroma）
    - **Cb**（U）：蓝色色度分量，占8bit（1字节）
    - **Cr**（V）：红色色度分量，占8bit（1字节）

### 转换公式
```
公式1：
- RGB的取值范围是[0,255]
- Y的取值范围是[16,235]
- UV的取值范围是[16,239]

Y = 0.257R + 0.504G + 0.098B + 16
U = -0.148R - 0.291G + 0.439B + 128
V = 0.439R - 0.368G - 0.071B + 128
 
R = 1.164(Y - 16) + 2.018(U - 128)
G = 1.164(Y - 16) - 0.813(V - 128) - 0.391(U - 128)
B = 1.164(Y - 16) + 1.596(V - 128)
```

```
公式2：
- RGB的取值范围是[0, 1]
- Y的取值范围是[0, 1]
- UV的取值范围是[-0.5, 0.5]

Y = 0.299R + 0.587G + 0.114B
U = 0.564(B - Y) = -0.169R - 0.331G + 0.500B
V = 0.713(R - Y) = 0.500R - 0.419G - 0.081B
 
R = Y + 1.403V
G = Y - 0.344U - 0.714V
B = Y + 1.770U
```

```
公式3：
- RGB的取值范围是[0, 255]
- YUV的取值范围是[0, 255]

Y = 0.299R + 0.587G + 0.114B
U = -0.169R - 0.331G + 0.500B + 128
V = 0.500R - 0.419G - 0.081B + 128
 
R = Y + 1.403(V - 128)
G = Y - 0.343(U - 128) - 0.714(V - 128)
B = Y + 1.770(U - 128)
```

### 色度二次采样
如果在色度分量上进行（相对亮度分量）较低分辨率的采样，也就是存储较多的亮度细节、较少的色度细节，这样就可以在不明显降低画面质量的同时减小图像的体积。上述过程称为：**色度二次采样**（Chroma Subsampling）。

#### 采样格式
采样格式通常用A:B:C的形式来表示，比如4:4:4、4:2:2、4:2:0等，其中我们最需要关注的是**4:2:0**。

比例N1∶ N2∶ N3 里面的数字指水平方向上的相对采样率， N1 表示在奇数行和偶数行里Y 样本的个数， N2 表示奇数行里Cb 和Cr 样本的个数， N3 是偶数行里Cb 和Cr 样本的个数

- **A**：一块A*2个像素的概念区域，一般都是4
- **B**：第1行的色度采样数目
- **C**：第2行的色度采样数目
    - C的值一般要么等于B，要么等于0
![[Pasted image 20230628195133.png]]
![[Pasted image 20230628195142.png]]

上图中，不管是哪种采样格式，Y分量都是全水平、全垂直分辨率采样的，每一个像素都有自己独立的Y分量。

#### 4:4:4
- 第1行采集4组CbCr分量，第2行采集4组CbCr分量
- 每1个像素都有自己独立的1组CbCr分量
    - Y分量与CbCr分量的水平方向比例是1:1（每1列都有1组CbCr分量）
    - Y分量与CbCr分量的垂直方向比例是1:1（每1行都有1组CbCr分量）
    - Y分量与CbCr分量的总比例是1:1
- 1个像素占用24bit（3字节），跟RGB888的体积一样
    - 24bpp（bits per pixel）
- 这种格式是没有进行色度二次采样的
![[497279-20210426104306964-2082053712.gif]]
叉叉代表：亮度。

圆圈代表：色度。


#### 4:2:2
在4∶ 2∶ 2 的采样模式中， 不仅对于奇数行里的每4 个亮度样本有2 个Cb 和Cr 样本， 而且对于偶数行里的每4 个亮度样本也有2 个Cb 和Cr 样本。

- 第1行采集2组CbCr分量，第2行采集2组CbCr分量
- 水平方向相邻的2个像素（1行2列）共用1组CbCr分量
    - Y分量与CbCr分量的水平方向比例是2:1（每2列就有1组CbCr分量）
    - Y分量与CbCr分量的垂直方向比例是1:1（每1行都有1组CbCr分量）
    - Y分量与CbCr分量的总比例是2:1
- 1个像素平均占用16bit（2字节）
    - 16bpp
    - 因为2个像素共占用32bit（4字节 = 2个Y分量 + 1个Cb分量 + 1个Cr分量）
![[497279-20210426104309834-1923088815.gif]]



#### 4:2:0
在4∶ 2∶ 0 的采样模式中， N1 =4， N2 =2， N3 =0。这意味着对于奇数行里的每4 个亮度样本， 有2 个Cb 样本和2 个Cr 样本； 但对于偶数行里的每4 个亮度样本， 没有Cb 和Cr 样本。

- 第1行采集2组CbCr分量，第2行共享第1行的CbCr分量
- 相邻的4个像素（2行2列）共用1组CbCr分量
    - Y分量与CbCr分量的水平方向比例是2:1（每2列就有1组CbCr分量）
    - Y分量与CbCr分量的垂直方向比例是2:1（每2行就有1组CbCr分量）
    - Y分量与CbCr分量的总比例是4:1
- 1个像素平均占用12bit（1.5字节）
    - 12bpp
    - 因为4个像素共占用48bit（6字节 = 4个Y分量 + 1个Cb分量 + 1个Cr分量）
![[497279-20210426104312115-357762309.gif]]
4:2:0 MPEG-1

![[497279-20210426110953281-1335685401.gif]]
4:2:0 MPEG-2
![[Pasted image 20230814161021.jpg]]




### 存储格式
YUV的存储格式可以分为3大类：
- **Planar**（平面）
    - Y、U、V分量分开单独存储
    - 名称通常以字母p结尾
- **Semi-Planar**（半平面）
    - Y分量单独存储，U、V分量交错存储
    - 名称通常以字母sp结尾
- **Packed**（紧凑）
    - 或者叫**Interleaved** （交错）
    - Y、U、V分量交错存储

![[Pasted image 20230628200818.png]]


```
Planar

1.I444

Y Y Y Y
Y Y Y Y
U U U U
U U U U
V V V V
V V V V

2.YV24

Y Y Y Y
Y Y Y Y
V V V V
V V V V
U U U U
U U U U

Semi-Planar

1.NV24

Y Y Y Y
Y Y Y Y
U V U V U V U V
U V U V U V U V

2.NV42
Y Y Y Y
Y Y Y Y
V U V U V U V U
V U V U V U V U


```

![[Pasted image 20230628201219.png]]

```
Planar
1.I422
Y Y Y Y
Y Y Y Y
U U
U U
V V
V V

2.YV16
Y Y Y Y
Y Y Y Y
V V
V V
U U
U U

Semi-Planar

1.NV16
Y Y Y Y
Y Y Y Y
U V U V
U V U V

2.NV61
Y Y Y Y
Y Y Y Y
V U V U
V U V U


```




****

# 视频压缩

使用一个视频而不做任何压缩是不可行的；**一个单独的一小时长的视频**，分辨率为720p和30fps时将**需要 278GB***。由于**仅使用独自无损数据压缩算法**，如 DEFLATE（被PKZIP, Gzip, 和 PNG 使用），**不会**足够减少所需的带宽，我们需要找到其它压缩视频的方法。
 >我们使用乘积得出这个数字 1280 x 720 x 24 x 30 x 3600 （宽，高，每像素比特数，fps 和秒数）

## 三角帧技术

==帧类型：I P B==
这是最开始的 4 帧。
![[Pasted image 20230504224739.png]]

以在帧内看到**很多重复内容**，如**蓝色背景**，从 0 帧到第 3 帧它都没有变化。为了解决这个问题，我们可以将它们**抽象地分类**为三种类型的帧。
#### **I 帧（帧内编码，关键帧）**

I 帧（可参考，关键帧，帧内编码）是一个**自足的帧**。它不依靠任何东西来渲染，I 帧与静态图片相似。第一帧通常是 I 帧，但我们将看到 I 帧被定期插入其它类型的帧之间。

#### **P 帧（预测）**
![[Pasted image 20230504224942.png]]

P 帧利用了一个事实：当前的画面几乎总能**使用之前的一帧进行渲染**。例如，在第二帧，唯一的改变是球向前移动了。仅仅使用（第二帧）对前一帧的引用和差值，我们就能重建前一帧。

#### **B 帧（双向预测）**

如何引用前面和后面的帧去做更好的压缩？！简单地说 B 帧就是这么做的。
![[Pasted image 20230504225129.png]]


 I 帧是昂贵的，P 帧是便宜的，最便宜的是 B 帧。
 ![[Pasted image 20230504225059.png]]



## 时间冗余（帧间预测）
包括==运动估计==和==运动补偿==

1. 简单地**从帧 0 里减去帧 1**，就可以得到刚好需要我们**去编码的剩余值**。
![[Pasted image 20230505131900.png]]
![[Pasted image 20230505131904.png]]

2. 块运动补偿
**块运动补偿**是将当前帧划分为非重叠的块，并且运动补偿向量**告诉这些块来自哪里**
![[Pasted image 20230505132052.png]]

预计那个球会从 `x=0, y=25` 移动到 `x=6, y=26`，**x** 和 **y** 的值就是**运动向量**。我们**下一步**可以通过只对在最终块的位置和预期值之间的**运动向量差进行编码**来节省比特，那么最终运动向量就是 `x=6 (6-0), y=1 (26-25)`。

运用**运动预测**时**编码的数据少于**使用简单的三角帧技术。（ 三角帧：在视频压缩技术里，P 帧或 B 帧的别名。）



## 空间冗余（帧内预测）
如果我们分析一个视频里的**每一帧**，我们会看到有**许多区域是相互关联的**。
![[Pasted image 20230505134351.png]]

这是一个 `I 帧`，并且我们**不能使用前面的帧来预测**，但仍然可以压缩它。我们将编码选择的红色块。如果我们**看看它的周围**，我们可以**估计它周围颜色的趋势**。
![[Pasted image 20230505134506.png]]

**预计**帧将继续**垂直传播（它的）颜色**，这意味着**未知像素的颜色将保持其邻居的值**。
![[Pasted image 20230505135028.png]]


我们的**预计也会错**，所以我们需要应用这项技术（**帧间预测**），然后**减去给我们的残差块的值**，得出一个相比原始数据可压缩的矩阵。
![[Pasted image 20230505134458.png]]


ffmpeg查看运动预测
```shell
ffmpeg -flags2 +export_mvs -i mv.mp4 -vf codecview=mv=pf+bf+bb mv_vis_mv.mp4
```



# 编码规范

从信息论的角度出发，根据解码后还原的数据是否与原始数据完全相同，可将数字图像与视频数据压缩编码方法分为两大类：无失真编码和限失真编码

无失真编码又称无损编码、统计编码、信息保持编码、熵编码
限失真编码也称有损编码、非信息保持编码、熵压缩编码。
![[Pasted image 20230731135615.png]]

10秒钟1080p（1920x1080）、30fps的YUV420P原始视频，需要占用多大的存储空间？
- (10 * 30) * (1920 * 1080) * 1.5 = 933120000字节 ≈ 889.89MB

需要先使用视频编码技术（比如H.264编码）对原始视频进行压缩，然后再进行存储和分发。H.264编码的压缩比可以达到至少是100:1。

1080p的p是指逐行扫描方式
1080I的I是指隔行扫描方式（还包括奇数行，偶数行），播放时出现锯齿一般是由于隔行扫描造成的

## H.264编码
H.264，又称为**MPEG-4 Part 10，Advanced Video Coding**。

H.264码流结构
![[Pasted image 20230918032636.png]]
H264 除了实现了对视频的压缩处理之外，为了方便网络传输，提供了对应的视频编码和 **分片** 策略。在 H264 中将其称为 **组(GOP, group of pictures)、片(slice)、宏块(Macroblock)** 。
    ![[Pasted image 20230918032732.png]]
    
H264 的码流分层结构组织成为 **序列(GOP)、图片(pictrue)、片(Slice)、宏块(Macroblock)、子块(subblock)** 五个层次。
![[Pasted image 20230918032743.png]]


NALU
- H264 原始码流(裸流)是由一个接一个 NALU 组成，它的功能分为两层，VCL(视频编码层)和 NAL(网络提取层)。
    - **VCL** 层，视频数据编码层(Video Coding Layer)，包括核⼼压缩引擎和块，宏块和片的语法级别定义，设计⽬标是尽可能地独立于网络进行高效的编码。
    - **NAL** 层，视频数据网络抽象层(Network Abstraction Layer)，负责将 **VCL** 产生的比特字符串适配到各种各样的网络和多元环境中，覆盖了所有片级以上的语法级别。

H264 有两种封装
![[Pasted image 20230918032616.png]]
![[Pasted image 20230918032623.png]]
- **annexb** 模式，传统模式，有 startcode，SPS 和 PPS 是在 ES 中。
- **mp4** 模式，一般 mp4、mkv 都是 mp4 模式，没有 startcode，SPS 和 PPS 以及其它信息被封装在 container 中，每一个 frame 前面 4 个字节是这个 frame 的长度。很多解码器只支持 annexb 这种模式，因此需要将 mp4 做转换。在 ffmpeg 中用 **h264_mp4toannexb_filter** 可以做转换。
#### SPS
[![SPS](https://github.com/gongluck/images/raw/main/av/H264/sps.png)](https://github.com/gongluck/images/blob/main/av/H264/sps.png)
- **序列参数集** ，保存了一组 **编码视频序列(Coded video sequence)** 的全局参数。
- SPS 主要包含的是图像的宽、高、YUV 格式和位深等基本信息。
#### PPS
- **图像参数集**，对应的是一个序列中某一幅图像或者某几幅图像的参数。
- PPS 主要包含 **熵编码** 类型、基础 **QP** 和 **最大参考帧数量** 等基本编码信息。

- 译为：**MPEG-4第10部分，高级视频编码**
- 简称：**MPEG-4 AVC**

基本的视频压缩原则：
1. 用于降低空间相关的变
2. 控制比特率的量化
3. 降低时间相关的运动补偿预测
4. 降低统计相关的熵编码、

新的和改进的方法罗列如下
1. 自适应帧内预测；
2. 整数精度的小尺寸块变换；
3. 多参考帧和广义B 帧；
4. 可变的块大小；
5. 1/4像素精度的运动补
6. 内容自适应环路去块效应滤波器；
7. 引入CABAC（上下文自适应二进制算术编码）和CAVLC（上下文自适应变长编码）改善的熵编码
## H· 264 的档次
档次定义为一个编码工具的子集
H· 264 所定义的档次如下所列：
1） 基本档次；
2） 扩展档次；
3） 主档次；
4） 高档次（定义在FRExts 修正案中）
![[Pasted image 20230701170327.png]]


![[Pasted image 20230701165949.png]]

![[Pasted image 20230701170626.png]]
H· 264 编码器框图

**H.264 采用的核心算法是『帧内压缩』和『帧间压缩』，帧内压缩是生成 I 帧的算法，帧间压缩是生成 B 帧和 P 帧的算法。**
## 编码过程与原理
大体可以归纳为以下几个主要步骤：

- 划分帧类型
- 帧内/帧间编码
- 变换 + 量化
- 滤波
- 熵编码


### 划分帧类型
在连续的几帧图像中，一般只有10%以内的像素有差别，亮度的差值变化不超过2%，而色度的差值变化只在1%以内。

#### GOP
==将一串连续的相似的帧归到一个图像群组（Group Of Pictures，**GOP**）==。**GOP（Group Of Pictures）是图像组的概念，它指的是视频编码序列中两个 I 帧之间的距离。**
GOP的第一帧必为I帧。使用GOP可以降低视频花掉的概率，断线重连后，直播流可以更快的重新播放
![[Pasted image 20230630164417.png]]

GOP中的帧可以分为3种类型：

- **I**帧（I Picture、I Frame、Intra Coded Picture），译为：**帧内编码图像**，也叫做关键帧（Keyframe）
    - **是视频的第一帧，也是GOP的第一帧**
    - 编码
        - 对整帧图像数据进行编码
    - 解码
        - 仅用当前I帧的编码数据就可以解码出完整的图像
    - 是一种==自带全部信息的独立帧==，无需参考其他图像便可独立进行解码，可以简单理解为一张静态图像

I 帧编码流程：

- 进行帧内预测，决定所采用的帧内预测模式；
- 当前像素值减去预测值，得到残差；
- 对残差进行变换和量化；
- 变长编码和算术编码；
- 重构图像并滤波，得到的图像作为其它帧的参考帧。

- **IDR帧**
**IDR 帧全称叫做 Instantaneous Decoder Refresh，是 I 帧的一种。IDR 帧的作用是立刻刷新，重新算一个新的序列开始编码，使错误不致传播。I 帧有被跨帧参考的可能，但 IDR 帧不会。
- IDR 帧一定是 I 帧，严格来说 I 帧不一定是 IDR 帧（但一般 I 帧就是 IDR 帧）；
    
- 对于 IDR 帧来说，在 IDR 帧之后的所有帧都不能引用任何 IDR 帧之前的帧的内容。与此相反，对于普通的 I 帧来说，位于其之后的 B 和 P 帧可以引用位于普通 I 帧之前的 I 帧（普通 I 帧有被跨帧参考的可能）；
    
- 播放器永远可以从一个 IDR 帧播放，因为在它之后没有任何帧引用之前的帧。因此，视频开头的 I 帧一定是 IDR 帧；一个封闭类 GOP 的开头的 I 帧也一定是 IDR 帧。


- **P**帧（P Picture、P Frame、Predictive Coded Picture），译为：**预测编码图像**
P 帧的预测与重构：P 帧是以 I 帧为参考帧，在 I 帧中找出 P 帧『某点』的预测值和运动矢量，取预测差值和运动矢量一起传送。在接收端根据运动矢量从 I 帧中找出 P 帧『某点』的预测值并与差值相加以得到 P 帧『某点』样值，从而可得到完整的 P 帧。
    - 编码
        - 并不会对整帧图像数据进行编码
        - ==以前面的I帧或P帧作为参考帧，只编码**当前P帧与参考帧**的**差异数据==**
    - 解码
        - 需要先解码出前面的参考帧，再结合差异数据解码出当前P帧完整的图像
P 帧特点：

- P 帧是 I 帧后面相隔 1-2 帧的编码帧；
    
- P 帧采用运动补偿的方法传送它与前面的 I 或 P 帧的差值及运动矢量（预测误差）；
    
- P 帧属于前向预测的帧间编码，它只参考前面最靠近它的 I 帧或 P 帧；
    
- P 帧可以是其后面 P 帧的参考帧，也可以是其前后的 B 帧的参考帧；
    
- 由于 P 帧是参考帧，它可能造成解码错误的扩散；
    
- 由于是差值传送，P 帧的压缩比较高。
    

P 帧编码的基本流程：

- 进行运动估计，计算采用帧间编码模式的率失真函数值。P 帧只参考前面的帧；
    
- 进行帧内预测，选取率失真函数值最小的帧内模式与帧间模式比较，确定采用哪种编码模式；
    
- 计算实际值和预测值的差值；
    
- 对残差进行变换和量化；
    
- 若编码，如果是帧间编码模式，编码运动矢量。


- **B**帧（B Picture、B Frame、Bipredictive Coded Picture），译为：**前后预测编码图像

B 帧，即双向预测编码图像帧，提供最高的压缩比，它既需要之前的图像帧（I 帧或 P 帧），也需要后来的图像帧（P 帧），采用运动预测的方式进行帧间双向预测编码。

B 帧的预测与重构：B 帧以前面的 I 或 P 帧和后面的 P 帧为参考帧，找出 B 帧『某点』的预测值和两个运动矢量，并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中找出预测值并与差值求和，得到 B 帧『某点』样值，从而可得到完整的 B 帧。
    编码
        并不会对整帧图像数据进行编码
        同时以前面、后面的I帧或P帧作为参考帧，只编码当前B帧与前后参考帧的差异数据
        因为可参考的帧变多了，所以只需要存储更少的差异数据
    解码
        需要先解码出前后的参考帧，再结合差异数据解码出当前B帧完整的图像

B 帧特点：

- B 帧是由前面的 I 或 P 帧和后面的 P 帧来进行预测的；
    
- B 帧传送的是它与前面的 I 或 P 帧和后面的 P 帧之间的预测误差及运动矢量；
    
- B 帧是双向预测编码帧；
    
- B 帧压缩比最高，因为它只反映两参考帧间运动主体的变化情况，预测比较准确；
    
- B 帧不是参考帧，不会造成解码错误的扩散。
    

B 帧编码的基本流程：

- 进行运动估计，计算采用帧间编码模式的率失真函数值。B 帧可参考后面的帧；
    
- 进行帧内预测，选取率失真函数值最小的帧内模式与帧间模式比较，确定采用哪种编码模式；
    
- 计算实际值和预测值的差值；
    
- 对残差进行变换和量化；
    
- 若编码，如果是帧间编码模式，编码运动矢量。


不难看出，编码后的数据大小：I帧 > P帧 > B帧。
![[Pasted image 20230630164601.png]]

在较早的视频编码标准（例如MPEG-2）中，P帧只能使用一个参考帧，而一些现代视频编码标准（比如H.264），允许使用多个参考帧。
![[Pasted image 20230630164640.jpg]]



### 帧内/帧间编码
- I帧采用的是帧内（Intra Frame）编码，处理的是空间冗余。  
- P帧、B帧采用的是帧间（Inter Frame）编码，处理的是时间冗余。

#### 划分宏块
在进行编码之前，首先要将一张完整的帧切割成多个宏块（Macroblock），H.264中的宏块大小通常是16x16。

宏块可以进一步拆分为多个更小的变换块（Transform blocks）、预测块（Prediction blocks）。

- 变换块的尺寸有：16x16、8x8、4x4
    
- 预测块的尺寸有：16×16、16×8、8×16、8×8、8×4、4×8、4×4

#### 帧内编码
帧内预测使用来自同一幅图像的宏块进行预测。对于亮度分量，使用两种类型的预测方案。这两个方案可以称为INTRA_4×4和INTRA_16×16。在INTRA_4×4中，一个大小为16×16的宏块分为16个4×4的子块。

4X4
帧内预测方案单独地运用到这些4×4的子块上。总共支持9种不同的预测模式如图所示
![[Pasted image 20230731144538.png]]
在模式0中，宏块样值用上面相邻的样值进行预测。在模式1中，宏块样值用左侧相邻的样值进行预测。在模式2中，所有相邻样值的均值用于预测。模式3沿左下方向对角线进行预测。模式4沿右下方向对角线进行预测。模式5是垂直向右方向。模式6是水平向下方向。模式7是垂直向左方向。模式8是水平向上方向。预测值是利用从A到M的预测值的加权平均计算得到

16×16
对于亮度分量的16×16帧内预测，用到4种模式（见图4·5）。模式0（垂直）、模式1（水平）和模式2（直流）这3种模式类似于4×4块的预测模式。在第4种模式中，相邻样值拟合了线性平面函数。
![[Pasted image 20230731144753.png]]

#### 帧间编码
H.264/AVC标准中的帧间预测是利用已编码视频帧/场和基于块的运动补偿的预测模式

##### 块大小可变的运动补偿
![[Pasted image 20230731162415.png]]

也就是说，一个宏块可以划分为多个不同大小的子块，每个子块都可以有单独的运动矢量。分块模式信息、运动矢量、预测误差都需要编码和传输。当选择比较大的块（如16×16，16×8，8×16）进行编码时，意味着块类型选择所用的比特数减少以及需要发送的运动矢量较少，但相应的运动补偿误差较大，因而需要编码的块残差数据较多；当采用较小的子块（如4×4，4×8，8×4）进行编码时，一个宏块需要传送更多的运动矢量，同时子块类型选择所用的比特数增加，比特流中宏块头信息和参数信息所占用的比特数大大增加，但是运动预测更加精确
  
因此，编码子块大小的选择对于压缩性能有比较大的影响。显然，对较大物体的运动，可采用较大的块来进行预测；而对较小物体的运动或细节丰富的图像区域，采用较小块运动预测的效果更加优良。


##### 高精度的亚像素运动估计
H.264较之H.263增强了运动估计的搜索精度。在H.263中采用的是半像素精度的运动估计，而在H.264中可以采用1/4甚至1/8像素精度的运动估计，
显然，运动矢量位移的精度越高，则帧间预测误差越小，数码率越低，即压缩比越高。

在H.264中，对于**亮度分量**，采用**1/4像素精度**的运动估计；对于**色度分量**，采用**1/8像素精度**的运动估计。
![[Pasted image 20230731163138.png]]
方块A～I代表了整数像素位置，a～h代表了半像素位置，1～8代表了1/4像素位置。

要进行1/4像素精度滤波，需要对图像进行插值以产生1/2、1/4像素位置处的样点值。在H.264中采用了6阶有限冲激响应滤波器的内插获得1/2像素位置的值。当1/2像素值获得后，1/4像素值可通过线性内插获得

##### 多参考帧的运动补偿预测
在MPEG⁃2、H.263等标准中，P帧只采用前一帧进行预测，B帧只采用相邻的两帧进行预测。而在H.264/AVC中，对P帧或者B帧编码时，最多可采用5个参考帧进行帧间预测，以此进一步提高运动补偿预测的精度。
![[Pasted image 20230731163228.png]]


#### 变换与量化
##### 变换

以前的标准MPEG-1和MPEG-2使用二维离散余弦变换（DCT）来实现大小为8×8块变换编码的目的，H·264采用一种**4×4整数变换**来对帧内预测和帧间预测的差值数据进行变换编码而不是DCT。

H·264中使用较小块尺寸的优势如下所述：
- 变换尺寸的降低使得编码器能够更好地让预测误差编码适应于运动对象的边界，而且用最小的运动补偿块尺寸匹配变换块尺寸；
- 较小的变换块尺寸导致振铃效应的明显降低；
- 基于整数运算的变换，其算法中只需要加法和移位运算，具有不需要乘法的优势。

  
H.264标准中的变换编码中根据差值数据类型的不同引入了**3种不同**的变换。
- 第一种用于16×16的帧内编码模式中**亮度块**的**DC系数重组的4×4矩阵**；
- 第二种用于16×16帧内编码模式中**色度块**的**DC系数重组的2×2矩阵**；
- 第三种是针对其他所有类型**4×4差值矩阵**。当采用自适应编码模式时，系统可以根据运动补偿采用不同的基本块大小进行变换。

![[Pasted image 20230801102933.png]]

##### 量化
H.264标准采用了分级量化模式

  
正向量化公式：
![[Pasted image 20230801103040.png]]
![[Pasted image 20230801103052.png]]
量化步长共分52个等级，由量化参数（QuantizationParameter，QP）值控制


#### 滤波
视频编解码器中加入去方块滤波器的方法有两种：后置滤波器和环路滤波器。

##### 环路去块效应滤波器
环路去块效应滤波器（见图4·3）用来去除因基于块编码模式而产生的块效应。
![[Pasted image 20230801103539.png]]

去块效应滤波器的操作可被分成三个主要的步骤，分别是滤波器强度计算、滤波器判决和滤波器实现。
###### 滤波器强度
H·264在以下3个级别上自适应地使用去块效应过程：
•在分片级———全局滤波强度调整到视频序列的独有特征；
•在块边沿级———去块效应滤波器判决基于块的帧间或帧内预测、运动差分和两个相关块编码残差的出现；•在样值级———区别块效应和图像真实的边沿是很重要的。真实边沿不应该被滤除掉。因而样值级别上对于去块操作的判决就变得很重要了。

### 熵编码
视频编码常用的有两种：变长编码（哈夫曼编码）、算术编码。
H.264 最后将结果进行熵编码，分为`上下文自适应的变长编码（Context-based Adaptive Variable-Length Coding，CAVLC）`与`上下文自适应的二进制算术编码（Context-based Adaptive Binary Arithmetic Coding，CABAC）`。



## H.265编码
![[Pasted image 20230808153438.png]]
![[Pasted image 20230808153453.png]]

#### 帧内预测

该模块主要用于去除图像的空间相关性。通过编码后的重构信息来预测当前像素块以去除空间冗余信息，提高图像的压缩效率。与以往的标准相比，**H.265 支持更多的帧内预测模式**。

#### 帧间预测
该模块主要用户去除图像的时间相关性。帧间预测通过将已编码的图像作为当前帧的参考图像，来获取各个块的运动信息，从而去除时间冗余，提高压缩效率。**在 H.265 中，帧间预测可采用单向和双向的参考图像来进行预测，包括类似 H.264 中分层 B 帧的预测结构**。

#### 变换和量化
该模块通过对残差数据进行变换量化以去除频域相关性，对数据进行有损压缩。变换编码将图像从时域信号变换至频域，将能量集中至低频区域。量化模块可以减小图像编码的动态范围。变换编码和量化模块从原理上属于两个相互独立的过程，**但是在 H.265 中，两个过程相互结合，减少了计算复杂度。量化部分整体和 H.264 相似，支持加权量化矩阵（自定义量化矩阵）**。

#### 环路滤波/去方块滤波（Deblocking）

去方块滤波（Deblocking）在基于块的视频编码中，形成的重构图像会出现方块效应，采用去方块滤波可达到削弱甚至消除方块效应的目的，提高图像的主观质量和压缩效率。**H.265 仍然是基于块的视频编码，因此延续了环内去方块滤波的思路。在 TU/PU 块边界进行滤波，根据 MV、QP 等决定不同滤波强度**。

#### 环路滤波/样点自适应补偿滤波（SAO）
样点自适应补偿滤波（Sample Adaptive Offset，SAO）处于去方块滤波之后，通过解析去方块滤波后的像素的统计特性，为像素添加相应的偏移值，可以在一定程度上削弱振铃效应，提高图像的主观质量和压缩效率。**SAO 是 H.265 新增的一项编码方式**。

#### 熵编码

该模块将编码控制数据、量化变换系数、帧内预测数据以及运动数据等编码为二进制流进行存储或传输。熵编码模块的输出数据即原始视频压缩后的码流。**H.265 中采用先进的基于上下文的自适应二进制算术编码（CABAC）进行熵编码，引入了并行处理架构（Slice/Tile、WPP），在速度、压缩率和内存占用等方面均得到了大幅改善**。

#### 特色的核心技术
H.265 的编码性能有了很大的提升，这源于新编码工具的使用以及自身具有特色的核心技术：

##### 编码单元
H.265 采用了编码树单元（Coding Tree Unit，CTU）和编码树块（Coding Tree Block，CTB）。H.265 中的 CTU 的概念类似于传统的宏块，但它的大小是可以由编码器设定的，并且可以超越 16x16。一个 CTU 由一个亮度 CTB、两个色度 CTB 和一些关联的语法元素组成。
为了更灵活有效地表示视频内容，H.265 为图像的划分定义了一套全新的分割模式：**灵活的四叉树划分结构，包括编码单元（Coding Unit，CU）、预测单元（Prediction Unit，PU）和变换单元（Transform Unit，TU）。这种特性有助于编码器根据视频内容特性、视频应用和终端特性来自适应地选择编码模式。**

编码单元的划分：

- 首先可以将图像均等划分为编码树单元（CTU），最大 64x64；
    
- CTU 可以根据实际编码决策，按照四叉树划分为更小的编码单元（CU）；
    
- 每一个叶节点的 CU 可以选择帧内编码或者帧间编码。
- 
##### 大尺寸离散余弦变换
**大尺寸离散余弦变换是 H.265 视频编码标准中提升编码效率的重要技术之一。在 H.264 中仅采用了 4x4/8x8 的 DCT 变换。而在 H.265 中 DCT 变换的最大尺寸为 32x32，这种大尺寸变换单元的选择可以使编码器在处理高分辨率画面中经常出现平坦区域时能够更好地提高压缩率。**

##### 改进的帧内预测技术

H.264 基于 4x4 大小的编码块采用 9 种预测模式，基于 16x16 大小的编码块采用 4 种预测模式。考虑高清视频纹理的多样性，只采用 H.264 中提供的几种帧内预测模式是远远不够的。为了更准确地反映纹理特性，降低预测误差，H.265 共提供了 **35 种帧内预测模式**，包括 33 种角度预测以及 DC 预测模式和 Planar 预测模式。**增加的预测模式可以更好地匹配视频中复杂的纹理，得到更好的预测效果，更加有效地去除空间冗余。**

##### 先进的帧间预测技术
为了提升帧间预测性能，**H.265 引入了新的帧间预测技术，包括运动信息融合技术（Merge）、先进的运动矢量预测技术（Advanced Motion Vector Predictor，AMVP）以及基于 Merge 的 Skip 模式**。




## 格式转换
### 其他图片格式转YUV
```shell
ffmpeg -i in.png -s 512x512 -pix_fmt yuv420p out.yuv|
```
上述命令生成的yuv文件大小是：393216字节 = 512 * 512 * 1.5字节。

- _-s_
    - 设置图片的尺寸
    - 可以用一些[固定字符串](https://ffmpeg.org/ffmpeg-all.html#Video-size)表示尺寸，比如**hd720**表示**1280x720**
    - 如果不设置此选项，默认会跟随输入图片的尺寸
- _-pix_fmt_
    - 设置像素格式
    - 可以通过`ffmpeg -pix_fmts`查看FFmpeg支持的像素格式
    - 如果不设置此选项，默认会跟随输入图片的像素格式
        - 比如可能是**rgb24**、**rgba8**、**pal8**等
        - 可以通过_ffprobe_查看某图片的像素格式，比如`ffprobe in.png`


### YUV转其他图片格式
```shell
ffmpeg -s 512x512 -pix_fmt yuv420p -i in.yuv out.jpg|
```
- 这里必须得设置YUV的尺寸（_-s_）、像素格式（_-pix_fmt_）
- 这就类似于：对pcm进行编码时，必须得设置采样率（_-ar_）、声道数（_-ac_）、采样格式（_-f_）

### 显示YUV
```shell
ffplay -video_size 512x512 -pixel_format yuv420p in.yuv

# 只显示r分量
ffplay -vf extractplanes=r in.png
 
# 只显示g分量
ffplay -vf extractplanes=g in.png
 
# 只显示b分量
ffplay -vf extractplanes=b in.png
 
# 只显示y分量
ffplay -video_size 512x512 -pixel_format yuv420p -vf extractplanes=y in.yuv
# 只显示y分量
ffplay -video_size 512x512 -pixel_format yuv420p -vf extractplanes=u in.yuv
# 只显示y分量
ffplay -video_size 512x512 -pixel_format yuv420p -vf extractplanes=v in.yuv

```

# 视频录制
```shell
### dshow支持的设备
ffmpeg -f dshow -list_devices true -i dummy

## 默认参数录制
ffmpeg -f dshow -i video="Integrated Camera" out.yuv
## 自定义参数录制
ffmpeg -f dshow -video_size 640x480 -pixel_format yuyv422 -framerate 30 -i video="Integrated Camera" out.yuv|

## 播放
ffplay -video_size 1280x720 -pixel_format yuvj422p -framerate 30 out.yuv
```




------------------------------ 音频 ---------------------------------


录音的原理可以简单理解为：把声源的振动记录下来，需要时再让某个物体按照记录下来的振动规律去振动，就会产生与原来一样的声音。

 # PCM
## 音频数字化
将**模拟信号**（Analog Signal）转成**数字信号**（Digital Signal）后进行存储

![[Pasted image 20230607093803.png]]

## 脉冲编码调制
将音频数字化的常见技术方案是**脉冲编码调制**（**PCM**，Pulse Code Modulation）
主要过程是：**采样 → 量化 → 编码**。
![[Pasted image 20230607093904.png]]

### 采样
模拟信号的波形是无限光滑的，可以看成由无数个点组成，由于存储空间是相对有限的，数字编码过程中，必须要对波形的点进行采样。**采样**（Sampling）：每隔一段时间采集一次模拟信号的样本，是一个在时间上将模拟信号离散化（把连续信号转换成离散信号）的过程。

#### 采样率
每秒采集的样本数量，称为**采样率**（采样频率，采样速率，Sampling Rate）。比如，采样率44.1kHz表示1秒钟采集44100个样本。

#### 采样定理
只有当采样率高于声音信号最高频率的**2**倍时，才能把采集的声音信号唯一地还原成原来的声音。人耳能够感觉到的最高声音频率为20000Hz，因此为了满足人耳的听觉要求，需要至少每秒进行40000次采样（40kHz采样率）。这就是为什么常见的CD的采样率为44.1kHz。电话、无线对讲机、无线麦克风等的采样率是8kHZ。

### 量化
**量化**（Quantization）：将每一个采样点的样本值数字化。

#### 位深度
**位深度**（采样精度，采样大小，Bit Depth）：使用多少个二进制位来存储一个采样点的样本值。位深度越高，表示的振幅越精确。常见的CD采用16bit的位深度，能表示65536（216）个不同的值。DVD使用24bit的位深度，大多数电话设备使用8bit的位深度。
![[Pasted image 20230607094146.png]]

### 编码
**编码**：将采样和量化后的数字数据转成二进制码流。

## PCM数据
采样率44.1kHZ、位深度16bit的1分钟立体声PCM数据有多大？

- 采样率 * 位深度 * 声道数 * 时间
- _44100 * 16 * 2 * 60 / 8 ≈ 10.34MB_

## 比特率
**比特率**（Bit Rate），指单位时间内传输或处理的比特数量，单位是：比特每秒（bit/s或bps），还有：千比特每秒（Kbit/s或Kbps）、兆比特每秒（Mbit/s或Mbps）、吉比特每秒（Gbit/s或Gbps）、太比特每秒（Tbit/s或Tbps）。

采样率44.1kHZ、位深度16bit的立体声PCM数据的比特率是多少？

- 采样率 * 位深度 * 声道数
- _44100 * 16 * 2 = 1411.2Kbps_

# 音频的编码与解码

## 编码
PCM数据可以理解为是：**未经压缩的原始音频数据**，体积比较大，为了更便于存储和传输，一般都会使用某种**音频编码**对它进行编码压缩，然后再存成某种**音频文件格式**。

压缩分为**无损**压缩和**有损**压缩。

- **无损**压缩
    - 解压后**可以**完全还原出原始数据
    - 压缩比**小**，体积**大**
- **有损**压缩
    - 解压后**不能**完全还原出原始数据，会丢失一部分信息
    - 压缩比**大**，体积**小**
    - 压缩比越大，丢失的信息就越多，还原后的信号失真就会越大
    - 一般是通过**舍弃原始数据中对人类听觉不重要的部分**，达成压缩成较小文件的目的
- 压缩比 = 未压缩大小 / 压缩后大小

## 解码
当需要播放音频时，得先解码（解压缩）出PCM数据，然后再进行播放。
![[Pasted image 20230607101343.png]]

# 常见的音频编码和文件格式

需要注意的是：**音频文件格式并不等于音频编码**。比如：

- **WAV**只是一种文件格式，并不是一种编码
    
- **FLAC**既是一种文件格式，又是一种编码

![[Pasted image 20230607101439.png]]


查看录音设备
```shell
ffmpeg -list_devices true -f dshow -i dummy
```

指定录音设备录音
```shell
ffmpeg -f dshow -i audio="耳 机 (EDIFIER W800BT Plus Hands-Free AG Audio)" out.wav
```

## WAV文件格式
- WAV、AVI文件都是基于RIFF标准的文件格式，开头都是RIFF
![[Pasted image 20230616090352.png]]
每一个chunk（数据块）都由3部分组成：
- **id**：chunk的标识
- **data size**：chunk的数据部分大小，字节为单位
- **data**，chunk的数据部分

整个WAV文件是一个RIFF chunk，它的data由3部分组成：

- **format**：文件类型
- **fmt chunk**
    - **音频参数**相关的chunk
    - 它的data里面有采样率、声道数、位深度等参数信息
- **data chunk**
    - **音频数据**相关的chunk
    - 它的data就是真正的音频数据（比如PCM数据）

RIFF chunk除去data chunk的data（音频数据）后，剩下的内容可以称为：WAV文件头，一般是44字节。


### PCM转WAV

#### 命令行
```shell
ffmpeg -ar 44100 -ac 2 -f s16le -i out.pcm out.wav
```
上面命令生成的WAV文件头有78字节。对比44字节的文件头，它多增加了一个34字节大小的LIST chunk。（存放FFmpeg版本等信息）

加上一个输出文件参数_-bitexact_可以去掉LIST Chunk。
```shell
ffmpeg -ar 44100 -ac 2 -f s16le -i out.pcm -bitexact out2.wav|
```

#### 编程
在PCM数据的前面插入一个44字节的WAV文件头，就可以将PCM转成WAV。
```c++
#define AUDIO_FORMAT_PCM 1
#define AUDIO_FORMAT_FLOAT 3
 
// WAV文件头（44字节）
typedef struct {
    // RIFF chunk的id
    uint8_t riffChunkId[4] = {'R', 'I', 'F', 'F'};
    // RIFF chunk的data大小，即文件总长度减去8字节
    uint32_t riffChunkDataSize;
 
    // "WAVE"
    uint8_t format[4] = {'W', 'A', 'V', 'E'};
 
    /* fmt chunk */
    // fmt chunk的id
    uint8_t fmtChunkId[4] = {'f', 'm', 't', ' '};
    // fmt chunk的data大小：存储PCM数据时，是16
    uint32_t fmtChunkDataSize = 16;
    // 音频编码，1表示PCM，3表示Floating Point
    uint16_t audioFormat = AUDIO_FORMAT_PCM;
    // 声道数
    uint16_t numChannels;
    // 采样率
    uint32_t sampleRate;
    // 字节率 = sampleRate * blockAlign
    uint32_t byteRate;
    // 一个样本的字节数 = bitsPerSample * numChannels >> 3
    uint16_t blockAlign;
    // 位深度
    uint16_t bitsPerSample;
 
    /* data chunk */
    // data chunk的id
    uint8_t dataChunkId[4] = {'d', 'a', 't', 'a'};
    // data chunk的data大小：音频数据的总长度，即文件总长度减去文件头的长度(一般是44)
    uint32_t dataChunkDataSize;
} WAVHeader;
```


### 音频重采样
#### 命令行
```shell
ffmpeg -ar 44100 -ac 2 -f s16le -i 44100_s16le_2.pcm -ar 48000 -ac 1 -f f32le 48000_f32le_1.pcm
```

#### 编程
```C++
swr_convert(ctx,
            outAudioData, outNbSamples,
            (const uint8_t **)inAudioData,
            inNbSamples)
```

## AAC编码

![[Pasted image 20230731142515.png]]
pcm转wav
```shell
 ffmpeg -f s16le -ar 44100 -ac 2 -i 44100_s16le_2.pcm 44100_s16le_2.wav
```

pcm转aac
```shell
ffmpeg -ar 44100 -ac 2 -f s16le -i in.pcm -c:a libfdk_aac out.aac
```

wav转aac
```shell
ffmpeg -i in.wav -c:a libfdk_aac out.aac
```

AAC编码的文件扩展名主要有3种：aac、m4a、mp4。
```shell
# m4a

ffmpeg -i in.wav -c:a libfdk_aac out.m4a

# mp4

ffmpeg -i in.wav -c:a libfdk_aac out.mp4
```

编码流程：
![[Pasted image 20230625092349.png]]
1.获取编码器: avcodec_find_encoder_by_name("libfdk_aac")

2.检查PCM格式是否被编码器支持

3.创建编码上下文: AVCodecContext *ctx = avcodec_alloc_context3(codec)

4.给上下文设置参数

5.打开编码器: avcodec_open2

6.创建AVFrame: av_frame_alloc

7.创建AVPacket: av_packet_alloc

8.打开文件

9.读取数据，放入AVFrame

10.把AVFrame数据放入编码器: avcodec_send_frame

11.从编码器中获取编码好的数据并放入AVPacket: avcodec_receive_packet

12.写入文件

13.全部读完以后刷新缓冲区

14.释放资源

