## 1.  虚函数的作用
C++中的虚函数的作用主要是实现了多态的机制。基类定义虚函数，子类可以重写该函数；在派生类中对基类定义的虚函数进行重写时，需要在派生类中声明该方法为虚方法。

当子类重新定义了父类的虚函数后，**当父类的指针指向子类对象的地址时**，[即B b; A a = &b;] 父类指针根据赋给它的不同子类指针，动态的调用子类的该函数，而不是父类的函数
且这样的函数调用发生在运行阶段，而不是发生在编译阶段，称为动态联编。而函数的重载可以认为是多态，只不过是静态的。注意，非虚函数静态联编，效率要比虚函数高，但是不具备动态联编能力。

## 2.  析构函数为什么是虚函数

删除指针对象是没有问题的，指针对象的析构函数会正确调用，但仅限于指针的类型所表示的对象大小。

如果以一个==基类指针指向其派生类==，==删除这个基类指针只能删除基类对象部分，而不能删除整个派生类对象==，原因是通过基类指针无法访问派生类的析构函数。

但是，如果像其它虚函数一样，基类的析构函数也是虚的，那么派生类的析构函数也必然是虚的，==**删除基类指针**时，它就会通过**虚函数表**找到正确的**派生类析构函数**并调用它，从而正确析构整个派生类对象。==


## 3.  如何检测内存泄漏

### 检测工具

在Linux平台，我们可以使用==valgrind==命令检测C/C++程序是否内存泄露。
Windows下可以使用**CRT库**


## 4.  如何解决内存泄漏

### 避免内存泄露的几种方式
-   计数法：使用new或者malloc时，让该数+1，delete或free时，该数-1，程序执行完打印这个计数，如果不为0则表示存在内存泄露
-   一定要将基类的析构函数声明为**虚函数**
-   对象数组的释放一定要用**delete []**
-   有new就有delete，有malloc就有free，保证它们一定成对出现
- 使用智能指针

## ==5. 对linux了解多少？比如socket编程等==




## 6. tcp如何保障传输的可靠性？丢包了怎么办？

TCP主要提供了检验和、序列号/确认应答、超时重传、最大消息长度、滑动窗口控制等方法实现了可靠性传输。

==超时重传==RTO[Retransmission Timeout]超时，TCP会重传数据包。TCP认为这种情况比较糟糕，反应也比较强烈：

-   由于发生丢包，将慢启动阈值ssthresh设置为当前cwnd的一半，即ssthresh = cwnd / 2.
-   cwnd重置为1
-   进入慢启动过程

当收到三个重复确认ACK时，TCP开启==快速重传==Fast Retransmit算法，而不用等到RTO超时再进行重传：

-   cwnd大小缩小为当前的一半
-   ssthresh设置为缩小后的cwnd大小
-   然后进入快速恢复算法Fast Recovery。



## 7.  描述一下快速排序的过程？

1.  从数列中挑出一个元素，称为 "基准"（pivot）;
    
2.  重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；
    
3.  递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；

![[quickSort.gif]]

```c++
 int Paritition1(int A[], int low, int high) {  
   int pivot = A[low];
   while (low < high) {  
     while (low < high && A[high] >= pivot) {  
       --high;  
     }  
     A[low] = A[high];  
     while (low < high && A[low] <= pivot) {  
       ++low;  
     }  
     A[high] = A[low];  
   }  
   A[low] = pivot;  
   return low;  
 }  
  
 void QuickSort(int A[], int low, int high) //快排母函数  
 {  
   if (low < high) {  
     int pivot = Paritition1(A, low, high);  
     QuickSort(A, low, pivot - 1);  
     QuickSort(A, pivot + 1, high);  
   }  
 }
```


## 8.  TCP 三次握手、四次挥手

### 三次握手
刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态，进行三次握手：

-   第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN(c)。此时客户端处于 `SYN_SEND` 状态。
    
    首部的同步位SYN=1，初始序号seq=x，SYN=1的报文段不能携带数据，但要消耗掉一个序号。
    
-   第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN(s)。同时会把客户端的 ISN + 1 作为ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于 `SYN_RCVD` 的状态。
    
    在确认报文段中SYN=1，ACK=1，确认号ack=x+1，初始序号seq=y。
    
-   第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 `ESTABLISHED` 状态。服务器收到 ACK 报文之后，也处于 `ESTABLISHED` 状态，此时，双方已建立起了连接。
    
 确认报文段ACK=1，确认号ack=y+1，序号seq=x+1（初始为seq=x，第二个报文段所以要+1），ACK报文段可以携带数据，不携带数据则不消耗序号。

在socket编程中
服务端：socket() -> bind()-> listen()-> accept() -> recv() / send()-> close()
客户端：socket() -> connect() -> send() / recv() -> close()

服务端listen:
- 申请和初始化接收队列（半连接队列与全连接队列），全连接队列为链表，半连接队列为hash表。
- 全连接队列长度为listen时传入的backlog与net.core.somaxconn之间的最小值。半连接队列为min(backlog, somaxconn, tcp_max_syn_backlog) + 1 再向上取整到2的N次幂(>16); 如min(backlog, somaxconn, tcp_max_syn_backlog) = 8 + 1 = 9 向上取整到16;- 

客户端执行connect()时，将触发三次握手。
connect():
- 进入内核的系统调用源码，根据socket文件描述符查询对应的socket内核对象。
- 设置socket状态为TCP_SYN_SEND，选择可用端口（生成随机数并进行遍历，直到找到可用端口），若遍历完没有发现，则返回`Cannot assign requested address`错误。即使端口被使用过，只要四元组（源地址、源端口，目的地址、目的端口）不同，也可以使用。
- 申请一个skb，设为SYN包，添加到发送队列进行发送，并启动重传定时器，超时没收到ACK则重发。

第二次握手，服务端响应SYN：
- 收到SYN数据包，经过网卡，软中断，进入tcp_v4_recv(), 判断SYN为第一次握手包。进行处理
- 判断半连接队列是否满了，满了则检查tcp_syncookies是否为1开启，若未开启则直接丢弃，若开启了，则发送syn = (t mod 32, MSS, hash(ip,port,t))，分别6bits, 2bits, 24bits。客户端收到后发送ack = syn + 1的第三次握手包，服务段对ack - 1再解码得到连接信息。  若半连接队列未满则检查全连接队列，若已满，且有未完成的半连接请求young_ack(半连接队列的计数器),则直接丢弃。
- 若半连接队列未满则发送synack,将request_sock加入半连接队列，开启==计时器==

第三次握手
- 若客户端一直未收到服务端的synack，则到达事件后再次重发第一次握手的数据包。
- 客户端收到synack后设置状态为established并发送ACK数据包。
- 服务端收到之后，再次判断全连接队列是否满，若满则直接丢弃，否则从半连接队列中删除，加入全连接队列中。

服务端调用accept() ，从全连接队列中取走。
![[Pasted image 20230421183332.png]]

### 握手异常的解决方法
1. 打开syncookie
2. 加大连接队列长度
3. 加快accept()速度
4. 尽快拒绝，避免客户端一直重发
5. 减少TCP连接次数，使用长连接

### 四次挥手

刚开始双方都处于 ESTABLISHED 状态，假如是客户端先发起关闭请求。四次挥手的过程如下：

-   第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于 `FIN_WAIT1` 状态。 即发出**连接释放报文段**（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN_WAIT1（终止等待1）状态，等待服务端的确认。
-   第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 `CLOSE_WAIT` 状态。 即服务端收到连接释放报文段后即发出**确认报文段**（ACK=1，确认号ack=u+1，序号seq=v），服务端进入CLOSE_WAIT（关闭等待）状态，此时的TCP处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入FIN_WAIT2（终止等待2）状态，等待服务端发出的连接释放报文段。
-   第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 `LAST_ACK` 的状态。 即服务端没有要向客户端发出的数据，服务端发出**连接释放报文段**（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），服务端进入LAST_ACK（最后确认）状态，等待客户端的确认。
-   第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 +1 作为自己 ACK 报文的确认号值，此时客户端处于 `TIME_WAIT` 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态，服务端收到 ACK 报文之后，就处于关闭连接了，处于 `CLOSED` 状态。 即客户端收到服务端的连接释放报文段后，对此发出**确认报文段**（ACK=1，seq=u+1，ack=w+1），客户端进入TIME_WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL后，客户端才进入CLOSED状态。

![[Pasted image 20230421191405.png]]


### 2MSL时间的理由
1. 避免客户端的最后一个ack数据包丢失，造成服务端无法正常关闭连接，进入CLOSE
2. 防止“已失效的连接请求报文段”出现在本连接中。


## 9.  time_wait和close_wait
1.  保证客户端发送的最后一个ACK报文段能够到达服务端。 这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。
2.  防止“已失效的连接请求报文段”出现在本连接中。 客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。

## 10.  TCP滑动窗口

如果每次传输数据都只能发送一个MSS，就需要等待接收方的ACK，这显然会极大的影响传输的速率。在发送数据的时候，最好的方式是一下将所有的数据全部发送出去，然后一起确认
![[Pasted image 20230421192531.png]]
TCP的包可以分为四种状态
-   发送和已确认的字节（蓝色部分）
-   已发送但尚未确认的字节（黄色部分）
-   未发送的字节和接收方准备接收的字节，即在缓冲区buffer中（绿色部分）
-   未发送且接收方未准备接收的字节，即已经在缓冲区，但是该部分数据还未被处理（灰色部分）

第②和第③部分加起来就刚好是接收方缓冲区大小，发送窗口的最左边缘由接收方的最后一个 _ACK_ 确认决定，而长度由接收方通告的窗口大小决定，它规定了当前发送方能发送的最大数据量。


## 11.  字节序，系统调用转换字节序
计算机硬件有两种储存数据的方式：大端字节序（big endian）和小端字节序（little endian）。

-   **大端字节序**：高位字节在前，低位字节在后，这是人类读写数值的方法。
-   **小端字节序**：低位字节在前，高位字节在后，即以`0x1122`形式储存。

`0x1234567`的大端字节序和小端字节序的写法如下图。
![[bg2016112201.gif]]
htons()  host to network short

htonl()   host to network long

ntohs()  network to host short

ntohl()   network to host long


The  **htonl**()  function  converts  the  ==unsigned  integer  hostlong from host byte order to network== byte order.

The **htons**() function converts the ==unsigned short integer hostshort from host byte order to network== byte order.

The **ntohl**() function converts the ==unsigned integer netlong from network byte order to host== byte order.

The **ntohs**() function converts the ==unsigned short integer netshort from network byte  order to host== byte order.





**"只有读取的时候，才必须区分字节序，其他情况都不用考虑。"**


## 12.  一个包从网卡到应用层经历了什么
![[Pasted image 20230422204245.png]]


### 1. 从网卡到内存
- 数据包从外面的网络进入物理网卡。如果目的地址不是该网卡，且该网卡没有开启混杂模式，该包会被网卡丢弃。  
- 网卡将数据包通过DMA（Direct Memory Access）即直接存储器访问的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化。
- 网卡通过硬件中断（IRQ）通知CPU，告诉它有数据来了  
- CPU根据中断表，调用已经注册的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数  
- 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉网卡下次再收到数据包直接写内存就可以了，不要再通知CPU了，这样可以提高效率，避免CPU不停的被中断。  
- 启动软中断。这步结束后，硬件中断处理函数就结束返回了。由于硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致CPU没法响应其它硬件的中断

### 2.  内存-网络模块-协议栈
RPS实现了数据流的hash归类，并把软中断的负载均衡分到各个cpu  
7： 内核中的ksoftirqd进程专门负责软中断的处理，当它收到软中断后，就会调用相应软中断所对应的处理函数，对于上面第6步中是网卡驱动模块抛出的软中断，ksoftirqd会调用网络模块的net_rx_action函数  （每个CPU都有一个ksoftirqd进程，每个CPU的硬中断都由对应的ksoftirqd进程处理）

8： net_rx_action调用网卡驱动里的poll函数来一个一个的处理数据包  

9： 在pool函数中，驱动会一个接一个的读取网卡写到内存中的数据包，内存中数据包的格式只有驱动知道  

10： 驱动程序将内存中的数据包转换成内核网络模块能识别的skb格式，然后调用napi_gro_receive函数  

11： napi_gro_receive会处理GRO相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈。然后判断是否开启了RPS，如果开启了，将会调用enqueue_to_backlog  

12： 在enqueue_to_backlog函数中，会将数据包放入CPU的softnet_data结构体的input_pkt_queue中，然后返回，如果input_pkt_queue满了的话，该数据包将会被丢弃，queue的大小可以通过net.core.netdev_max_backlog来配置  

13： CPU会接着在自己的软中断上下文中处理自己input_pkt_queue里的网络数据（调用__netif_receive_skb_core）  

14： 如果没开启RPS，napi_gro_receive会直接调用__netif_receive_skb_core  

15： 看是不是有AF_PACKET类型的socket（也就是我们常说的原始套接字），如果有的话，拷贝一份数据给它。tcpdump抓包就是抓的这里的包。  

16： 调用协议栈相应的函数，将数据包交给协议栈处理。  

17： 待内存中的所有数据包被处理完成后（即poll函数执行完成），启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知CPU

### 3. 协议栈处理
1. 网络层
-  IP 层的入口函数在 ip_rcv 函数。该函数首先会做包括 package checksum 在内的各种检查，如果需要的话会做 IP defragment（将多个分片合并），然后 packet 调用已经注册的 Pre-routing netfilter hook ，完成后最终到达 ip_rcv_finish 函数。
-  ip_rcv_finish 函数会调用 ip_router_input 函数，进入路由处理环节。它首先会调用 ip_route_input 来更新路由，然后查找 route，决定该 package 将会被发到本机还是会被转发还是丢弃：
    -   如果是发到本机的话，调用 ip_local_deliver 函数，可能会做 de-fragment（合并多个 IP packet），然后调用 ip_local_deliver 函数。该函数根据 package 的下一个处理层的 protocal number，调用下一层接口，包括 tcp_v4_rcv （TCP）, udp_rcv （UDP），icmp_rcv (ICMP)，igmp_rcv(IGMP)。对于 TCP 来说，函数 tcp_v4_rcv 函数会被调用，从而处理流程进入 TCP 栈。
    -   如果需要转发 （forward），则进入转发流程。该流程需要处理 TTL，再调用 dst_input 函数。该函数会
        -   （1）处理 Netfilter Hook
        -   （2）执行 IP fragmentation
        -   （3）调用 dev_queue_xmit，进入链路层处理流程。


2. 传输层
-  传输层 TCP包的 处理入口在 tcp_v4_rcv 函数（位于 linux/net/ipv4/tcp ipv4.c 文件中），它会做 TCP header 检查等处理。
-  调用 tcp v4 lookup，查找该 package 的 open socket。如果找不到，该 package 会被丢弃。接下来检查 socket 和 connection 的状态。
-  如果socket 和 connection 一切正常，调用 tcp_prequeue 使 package 从内核进入 user space，放进 socket 的 receive queue。然后 socket 会被唤醒，调用 system call，并最终调用 tcp_recvmsg 函数去从 socket recieve queue 中获取 segment。

### 4. 应用层
1.  每当用户应用调用 read 或者 recvfrom 时，该调用会被映射为/net/socket.c 中的 sys_recv 系统调用，并被转化为 sys_recvfrom 调用，然后调用 sock_recgmsg 函数。
2.  对于 INET 类型的 socket，/net/ipv4/af inet.c 中的 inet_recvmsg 方法会被调用，它会调用相关协议的数据接收方法。
3.  对 TCP 来说，调用 tcp_recvmsg。该函数从 socket buffer 中拷贝数据到 user buffer。
4.  对 UDP 来说，从 user space 中可以调用三个 system call recv()/recvfrom()/recvmsg() 中的任意一个来接收 UDP package，这些系统调用最终都会调用内核中的 udp_recvmsg 方法。



## 13.  epoll两种事件触发模式
epoll的两种触发模式分别是ET(edge trigger)边缘触发和LT(level triggered)水平触发。  
epoll的默认触发模式是LT，select、poll只支持LT触发。

### LT

缓冲区只要有数据未读就会导致epoll_wait返回。  
上次读数据未读完仍会导致epoll_wait返回。  
水平触发模式下阻塞和非阻塞并没有什么区别，因为没有可读时间就绪的话epoll_wait不会返回。

### ET（状态改变触发）

缓冲区出现新未读数据才会导致epoll_wait返回。  
上次读数据未读完不会导致epoll_wait返回。


## 14.  TopK问题，时间复杂度，除了堆还有其他方法吗？



## 15.  x86-64的函数调用

![[Pasted image 20230422210130.png]]



## 16.  进程、线程、协程


|          | 进程 | 线程 | 协程 |
| -------- | ---- | ---- | ---- |
| 定义     |资源分配和拥有的基本单位|程序执行的基本单位|用户态的轻量级线程，线程内部调度的基本单位|
| 切换情况 |进程CPU环境(栈、寄存器、页表和文件句柄等)的保存以及新调度的进程CPU环境的设置|保存和设置程序计数器、少量寄存器和栈的内容| 先将寄存器上下文和栈保存，等切换回来的时候再进行恢复|
| 切换者   |操作系统|操作系统|用户|
| 切换过程 |用户态->内核态->用户态|用户态->内核态->用户态|用户态|
| 调用栈   |内核栈|内核栈|用户栈|
| 拥有资源 |     CPU资源、内存资源、文件资源和句柄等 |   程序计数器、寄存器、栈和状态字   |拥有自己的寄存器上下文和栈|
| 并发性   |  不同进程之间切换实现并发，各自占有CPU实现并行    | 一个进程内部的多个线程并发执行     |同一时间只能执行一个协程，而其他协程处于休眠状态，适合对任务进行分时处理      |
| 系统开销 |   切换虚拟地址空间，切换内核栈和硬件上下文，CPU高速缓存失效、页表切换，开销很大   |  切换时只需保存和设置少量寄存器内容，因此开销很小    | 直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快     |
| 通信         |  进程间通信需要借助操作系统    | 线程间可以直接读写进程数据段(如全局变量)来进行通信     |   共享内存、消息队列   |

进程是资源调度的基本单位，运行一个可执行程序会创建一个或多个进程，进程就是运行起来的可执行程序。我们编写的代码只是⼀个存储在硬盘的静态⽂件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这个可执⾏⽂件后，它会被装载到内存中，接着 CPU 会执⾏程序中的每⼀条指令，那么这个运⾏中的程序，就被称为==「进程」（Process）。==

线程是程序执行的基本单位，是轻量级的进程。每个进程中都有唯一的主线程，且只能有一个，主线程和进程是相互依存的关系，主线程结束进程也会结束。

协程是用户态的轻量级线程，线程内部调度的基本单位




## 17.  分布式算法了解哪些

## 18.  进程间通信
-   管道：
    -   ==无名管道==（内存文件）：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程之间使用。进程的亲缘关系通常是指父子进程关系。
    -   有名管道（FIFO文件，借助文件系统）：有名管道也是半双工的通信方式，但是允许在没有亲缘关系的进程之间使用，管道是先进先出的通信方式。

-   共享内存：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的IPC方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与信号量，配合使用来实现进程间的同步和通信。
    
-   消息队列：消息队列是有消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
    
-   套接字(socket)：适用于不同机器间进程通信，在本地也可作为两个进程通信的方式。
    
-   信号：用于通知接收进程某个事件已经发生，比如按下ctrl + C就是信号。
    
-   信号量：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，实现进程、线程的对临界区的同步及互斥访问。

## 19.  哈希与重哈希
### 哈希

#### 常用HASH函数
散列函数能使对一个数据序列的访问过程更加迅速有效，通过散列函数，数据元素将被更快地定位。常用Hash函数有：
##### 1．直接寻址法。
取关键字或关键字的某个线性函数值为散列地址。即H(key)=key或H(key) = a·key + b，其中a和b为常数（这种散列函数叫做自身函数）
##### 2．数字分析法。
分析一组数据，比如一组员工的出生年月日，这时我们发现出生年月日的前几位数字大体相同，这样的话，出现冲突的几率就会很大，但是我们发现年月日的后几位表示月份和具体日期的数字差别很大，如果用后面的数字来构成散列地址，则冲突的几率会明显降低。因此数字分析法就是找出数字的规律，尽可能利用这些数据来构造冲突几率较低的散列地址。
##### 3．平方取中法。
取关键字平方后的中间几位作为散列地址。
##### 4．折叠法。
将关键字分割成位数相同的几部分，最后一部分位数可以不同，然后取这几部分的叠加和（去除进位）作为散列地址。
##### 5．随机数法。
选择一随机函数，取关键字作为随机函数的种子生成随机值作为散列地址，通常用于关键字长度不同的场合。
##### 6．除留余数法。
取关键字被某个不大于散列表表长m的数p除后所得的余数为散列地址。即 H(key) = key MOD p,p<=m。不仅可以对关键字直接取模，也可在折叠、平方取中等运算之后取模。对p的选择很重要，一般取素数或m，若p选的不好，容易产生碰撞。


### 处理冲突方法
##### 1．开放寻址法；
Hi=(H(key) + di) MOD m,i=1,2,…，k(k<=m-1)，其中H(key)为散列函数，m为散列表长，di为增量序列，可有下列三种取法：
	1)． di=1,2,3,…，m-1，称==线性探测再散列==；
	2)． di=1^2,-1^2,2^2,-2^2,3^2,…，±k^2,(k<=m/2)称==二次探测再散列==；
	3)． di=伪随机数序列，称==伪随机探测再散列==。
##### 2． 再散列法：
Hi=RHi(key),i=1,2,…，k RHi均是不同的散列函数，即在同义词产生地址冲突时计算另一个散列函数地址，直到冲突不再发生，这种方法不易产生“聚集”，但增加了计算时间。
##### 3． 链地址法(拉链法)
##### 4． 建立一个公共溢出区

### 常用hash算法的介绍：
#### （1）MD4
MD4(RFC 1320)是 MIT 的Ronald L. Rivest在 1990 年设计的，MD 是 Message Digest（消息摘要） 的缩写。它适用在32位字长的处理器上用高速软件实现——它是基于 32位操作数的位操作来实现的。
#### （2）MD5
MD5(RFC 1321)是 Rivest 于1991年对MD4的改进版本。它对输入仍以512位分组，其输出是4个32位字的级联，与 MD4 相同。MD5比MD4来得复杂，并且速度较之要慢一点，但更安全，在抗分析和抗差分方面表现更好。
#### （3）SHA-1及其他


## 20.  一致性哈希
https://xiaolincoding.com/os/8_network_system/hash.html

大多数网站背后肯定不是只有一台服务器提供服务，因为单机的并发量和数据量都是有限的，所以都会用多台服务器构成集群来对外提供服务。

但是问题来了，现在有那么多个节点（后面统称服务器为节点，因为少一个字），要如何分配客户端的请求呢？

其实这个问题就是「负载均衡问题」。解决负载均衡问题的算法很多，不同的负载均衡算法，对应的就是不同的分配策略，适应的业务场景也不同。

最简单的方式，引入一个中间的负载均衡层，让它将外界的请求「轮流」的转发给内部的集群。比如集群有 3 个节点，外界请求有 3 个，那么每个节点都会处理 1 个请求，达到了分配请求的目的。

考虑到每个节点的硬件配置有所区别，我们可以引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求，这种算法叫做加权轮询。

加权轮询算法使用场景是建立在每个节点存储的数据都是相同的前提。所以，每次读数据的请求，访问任意一个节点都能得到结果。

但是，加权轮询算法是无法应对「分布式系统（数据分片的系统）」的，因为分布式系统中，每个节点存储的数据是不同的。

当我们想提高系统的容量，就会将数据水平切分到不同的节点来存储，也就是将数据分布到了不同的节点。比如**一个分布式 KV（key-valu） 缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的**，不是说任意访问一个节点都可以得到缓存结果的。

**哈希算法**。因为对同一个关键字进行哈希计算，每次计算都是相同的值，这样就可以将某个 key 确定到一个节点了，可以满足分布式系统的负载均衡需求。

但是有一个很致命的问题，**如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据**，否则会出现查询不到数据的问题。

要解决这个问题的办法，就需要我们进行**迁移数据**，比如节点的数量从 3 变化为 4 时，要基于新的计算公式 hash(key) % 4 ，重新对数据和节点做映射。

假设总数据条数为 M，哈希算法在面对节点数量变化时，**最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 O(M)**，这样数据的迁移成本太高了。

所以，我们应该要重新想一个新的算法，来避免分布式系统在扩容或者缩容时，发生过多的数据迁移。

一致性哈希算法就很好地解决了分布式系统在扩容或者缩容时，发生过多的数据迁移的问题。

一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而**一致哈希算法是对 2^32 进行取模运算，是一个固定的值**。

不同的负载均衡算法适用的业务场景也不同的。

轮询这类的策略只能适用与每个节点的数据都是相同的场景，访问任意节点都能请求到数据。但是不适用分布式系统，因为分布式系统意味着数据水平切分到了不同的节点上，访问数据的时候，一定要寻址存储该数据的节点。

哈希算法虽然能建立数据和节点的映射关系，但是每次在节点数量发生变化的时候，最坏情况下所有数据都需要迁移，这样太麻烦了，所以不适用节点数量变化的场景。

为了减少迁移的数据量，就出现了一致性哈希算法。

一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。

但是一致性哈希算法不能够均匀的分布节点，会出现大量请求都集中在一个节点的情况，在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。

为了解决一致性哈希算法不能够均匀的分布节点的问题，就需要引入虚拟节点，对一个真实节点做多个副本。不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。

引入虚拟节点后，可以会提高节点的均衡度，还会提高系统的稳定性。所以，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。


## 21.  字节对齐

### 为什么要内存对齐?

#### 平台原因(移植原因)：
不是所有的硬件平台都能访问任意地址上的任意数据的；某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常。
#### 性能原因：
数据结构(尤其是栈)应该尽可能地在自然边界上对齐。原因在于，为了访问未对齐的内存，处理器需要作两次内存访问；而对齐的内存访问仅需要一次访问。 假如没有内存对齐机制，数据可以任意存放，现在一个int变量存放在从地址1开始的联系四个字节地址中，该处理器去取数据时，要先从0地址开始读取第一个4字节块,剔除不想要的字节（0地址）,然后从地址4开始读取下一个4字节块,同样剔除不要的数据（5，6，7地址）,最后留下的两块数据合并放入寄存器。这需要做很多工作。 现在有了内存对齐的，int类型数据只能存放在按照对齐规则的内存中，比如说0地址开始的内存。那么现在该处理器在取数据时一次性就能将数据读出来了，而且不需要做额外的操作，提高了效率。
如果没有采用内存对齐: a占一个字节, b占4个字节.假如没有内存对齐这一说.当程序访问这一结构体变量时,变量位于物理内存中.那么假设a占据内存地址0,b则占据内存地址1~4.由于ram访问是每次读4bytes,那么我们想要访问b的时候,需要先读地址0~3,再读地址4~7,然后把地址1~3和地址4处的值拼出变量b的值放到寄存器里.这样在没有内存对齐的情况下,我们需要读两次内存,并且还需要进行相应的转换. 采用内存对齐: 那么如果我们采用内存对齐呢,编译的时候编译器给a分配0~3的地址,b分配4~7的地址,这样我们想要读b的值时,只需要读一次地址4~7就可以了.是不是节省了很多的cpu指令周期呀. 


### 内存对齐规则 
- 基本类型的对齐值就是其sizeof值; 
- 数据成员对齐规则：结构(struct)(或联合(union))的数据成员，第一个数据成员放在offset为0的地方，以后每个数据成员的对齐按照#pragma pack指定的数值和这个数据成员自身长度中，比较小的那个进行; 
- 结构(或联合)的整体对齐规则：在数据成员完成各自对齐之后，结构(或联合)本身也要进行对齐，对齐将按照#pragma pack指定的数值和结构(或联合)最大数据成员长度中，比较小的那个进行; 


内存对齐最最底层的原因是内存的IO是以8个字节64bit为单位进行的。 对于64位数据宽度的内存，假如cpu也是64位的cpu（现在的计算机基本都是这样的），每次内存IO获取数据都是从同行同列的8个chip中各自读取一个字节拼起来的。从内存的0地址开始，0-7字节的数据可以一次IO读取出来，8-15字节的数据也可以一次读取出来。
```c++
#include<iostream>

using namespace std;

class MyClass1
{
public:
	int m_A;
	virtual void Print(void);
	virtual void test(void);
	
protected:
	int m_B;
private:
	int m_C;
    int m_D;
};

class MyClass2
{
public:
	int m_A;
	void Print(void);
protected:
	int m_B;
private:
	int m_C;
};

class Son1 : public MyClass1
{
public:
	void Print(void);

};

class Son2 : public MyClass2
{
public:
	void Print(void);

};

void test1()
{

	cout << "sizeof MyClass1:" << sizeof(MyClass1) << endl;
	cout << "sizeof Son1:" << sizeof(Son1) << endl;

	cout << "sizeof MyClass2:" << sizeof(MyClass2) << endl;
	cout << "sizeof Son2:" << sizeof(Son2) << endl;
}
int main()
{
	test1();
	return 0;
}
```

sizeof MyClass1:24
sizeof Son1:24
sizeof MyClass2:12
sizeof Son2:12

### 伪共享问题
CPU 从内存中读取数据到 Cache 的时候，并不是⼀个字节⼀个字节读取，⽽是⼀块⼀块的⽅式来读取数据的，这⼀块⼀块的数据被称为 CPU Line（缓存⾏），所以 CPU Line 是 CPU 从内存读取数据到 Cache的单位。

假设有⼀个双核⼼的 CPU，这两个 CPU 核⼼并⾏运⾏着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是类型为 long 的变量 A 和 B，这个两个数据的地址在物理内存上是连续的，如果Cahce Line 的⼤⼩是 64 字节，并且变量 A 在 Cahce Line 的开头位置，那么这两个数据是位于同⼀个
Cache Line 中，⼜因为 CPU Line 是 CPU 从内存读取数据到 Cache 的单位，所以这两个数据会被同时读⼊到了两个 CPU 核⼼中各⾃ Cache 中。
![[Screenshot from 2023-04-22 22-16-49.png]]

如果 1 号和 2 号 CPU 核⼼这样持续交替的分别修改变量 A 和 B，就会不断修改MISE状态，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属于⼀个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从⽽出现不断修改MISE状态。

因此，这种因为多个线程同时读写同⼀个 Cache Line 的不同变量时，⽽导致 CPU Cache 失效的现象称为伪共享（False Sharing）。

因此采用内存对齐解决伪共享问题


## 22.  哪些负载均衡算法

### 1、轮询法
将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。

### 2、随机法
通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。

### 3、源地址哈希法
源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。

### 4、加权轮询法
不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。

### 5、加权随机法
与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。

### 6、最小连接数法
最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前
积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。


## 23.  Redis和MySQL用过吗？

## 24.  MySQL数据备份

mysql按照备份恢复方式分为==**逻辑备份**==和==**物理备份**==

逻辑备份是备份sql语句，在恢复的时候执行备份的sql语句实现数据库数据的重现

物理备份就是备份数据文件了，比较形象点就是cp下数据文件，但真正备份的时候自然不是的cp这么简单

这2种备份各有优劣，一般来说，物理备份恢复速度比较快，占用空间比较大，逻辑备份速度比较慢，占用空间比较小

**mysqldump工具备份**

mysqldump由于是mysql自带的备份工具，所以也是最常用的mysql数据库的备份工具。支持基于InnoDB的热备份。但由于是逻辑备份，所以速度不是很快，适合备份数据量比较小的场景。

**基于LVM快照备份**

在物理备份中 ，有基于文件系统的物理备份（LVM的快照），也可以直接用tar之类的命令打包。但这些只能进行冷备份

不同的存储引擎能备份的级别也不一样，MyISAM能备份到表级别，而InnoDB不开启每表一文件的话就只能备份整个数据库。


## 25. 详细说一下C++智能指针；

### 原理
符合RAII（资源获取即初始化）

智能指针是一个类，用来存储指向动态分配对象的指针，负责自动释放动态分配的对象，防止堆内存泄漏。动态分配的资源，交给一个类对象去管理，当类对象声明周期结束时，自动调用析构函数释放资源

### 常用的智能指针

**(1) shared_ptr**

实现原理：采用引用计数器的方法，允许多个智能指针指向同一个对象，每当多一个指针指向该对象时，指向该对象的所有智能指针内部的引用计数加1，每当减少一个智能指针指向对象时，引用计数会减1，当计数为0的时候会自动的释放动态分配的资源。

-   智能指针将一个计数器与类指向的对象相关联，引用计数器跟踪共有多少个类对象共享同一指针
-   每次创建类的新对象时，初始化指针并将引用计数置为1
-   当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数
-   对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数
-   调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）

**(2) unique_ptr**

unique_ptr采用的是独享所有权语义，一个非空的unique_ptr总是拥有它所指向的资源。转移一个unique_ptr将会把所有权全部从源指针转移给目标指针，源指针被置空；所以unique_ptr不支持普通的拷贝和赋值操作，不能用在STL标准容器中；局部变量的返回值除外（因为编译器知道要返回的对象将要被销毁）；如果你拷贝一个unique_ptr，那么拷贝结束后，这两个unique_ptr都会指向相同的资源，造成在结束时对同一内存指针多次释放而导致程序崩溃。

**(3) weak_ptr**

weak_ptr：弱引用。 引用计数有一个问题就是互相引用形成环（环形引用），这样两个指针指向的内存都无法释放。需要使用weak_ptr打破环形引用。weak_ptr是一个弱引用，它是为了配合shared_ptr而引入的一种智能指针，它指向一个由shared_ptr管理的对象而不影响所指对象的生命周期，也就是说，它只引用，不计数。如果一块内存被shared_ptr和weak_ptr同时引用，当所有shared_ptr析构了之后，不管还有没有weak_ptr引用该内存，内存也会被释放。所以weak_ptr不保证它指向的内存一定是有效的，在使用之前使用函数lock()检查weak_ptr是否为空指针。

**(4) auto_ptr**

主要是为了解决“有异常抛出时发生内存泄漏”的问题 。因为发生异常而无法正常释放内存。

auto_ptr有拷贝语义，拷贝后源对象变得无效，这可能引发很严重的问题；而unique_ptr则无拷贝语义，但提供了移动语义，这样的错误不再可能发生，因为很明显必须使用std::move()进行转移。

auto_ptr不支持拷贝和赋值操作，不能用在STL标准容器中。STL容器中的元素经常要支持拷贝、赋值操作，在这过程中auto_ptr会传递所有权，所以不能在STL中使用。


### 智能指针的作用
[[C++基础语法#]]
1.  C++11中引入了智能指针的概念，方便管理堆内存。使用普通指针，容易造成堆内存泄露（忘记释放），二次释放，程序发生异常时内存泄露等问题等，使用智能指针能更好的管理堆内存。
    
2.  智能指针在C++11版本之后提供，包含在头文件`<memory>`中，shared_ptr、unique_ptr、weak_ptr。shared_ptr多个指针指向相同的对象。shared_ptr使用引用计数，每一个shared_ptr的拷贝都指向相同的内存。每使用他一次，内部的引用计数加1，每析构一次，内部的引用计数减1，减为0时，自动删除所指向的堆内存。shared_ptr内部的引用计数是线程安全的，但是对象的读取需要加锁。
    
3.  初始化。智能指针是个模板类，可以指定类型，传入指针通过构造函数初始化。也可以使用make_shared函数初始化。不能将指针直接赋值给一个智能指针，一个是类，一个是指针。例如`std::shared_ptr<int> p4 = new int(1);`的写法是错误的

拷贝和赋值。拷贝使得对象的引用计数增加1，赋值使得原对象引用计数减1，当计数为0时，自动释放内存。后来指向的对象引用计数加1，指向后来的对象

4.  unique_ptr“唯一”拥有其所指对象，同一时刻只能有一个unique_ptr指向给定对象（通过禁止拷贝语义、只有移动语义来实现）。相比与原始指针unique_ptr用于其RAII的特性，使得在出现异常的情况下，动态资源能得到释放。unique_ptr指针本身的生命周期：从unique_ptr指针创建时开始，直到离开作用域。离开作用域时，若其指向对象，则将其所指对象销毁(默认使用delete操作符，用户可指定其他操作)。unique_ptr指针与其所指对象的关系：在智能指针生命周期内，可以改变智能指针所指对象，如创建智能指针时通过构造函数指定、通过reset方法重新指定、通过release方法释放所有权、通过移动语义转移所有权。
    
5.  智能指针类将一个计数器与类指向的对象相关联，引用计数跟踪该类有多少个对象共享同一指针。每次创建类的新对象时，初始化指针并将引用计数置为1；当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数；对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数；调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）。
6.  weak_ptr 是一种不控制对象生命周期的智能指针, 它指向一个 shared_ptr 管理的对象. 进行该对象的内存管理的是那个强引用的 shared_ptr. weak_ptr只是提供了对管理对象的一个访问手段。weak_ptr 设计的目的是为配合 shared_ptr 而引入的一种智能指针来协助 shared_ptr 工作, 它只可以从一个 shared_ptr 或另一个 weak_ptr 对象构造, 它的构造和析构不会引起引用记数的增加或减少




## 26. 说说你了解的auto_ptr作用；
1.  auto_ptr的出现，主要是为了解决“有异常抛出时发生内存泄漏”的问题；抛出异常，将导致指针p所指向的空间得不到释放而导致内存泄漏；
    
2.  auto_ptr构造时取得某个对象的控制权，在析构时释放该对象。我们实际上是创建一个`auto_ptr<Type>`类型的局部对象，该局部对象析构时，会将自身所拥有的指针空间释放，所以不会有内存泄漏；
    
3.  auto_ptr的构造函数是explicit，阻止了一般指针隐式转换为 auto_ptr的构造，所以不能直接将一般类型的指针赋值给auto_ptr类型的对象，必须用auto_ptr的构造函数创建对象；
    
4.  由于auto_ptr对象析构时会删除它所拥有的指针，所以使用时避免多个auto_ptr对象管理同一个指针；
    
5.  Auto_ptr内部实现，析构函数中删除对象用的是delete而不是delete[]，所以auto_ptr不能管理数组；
    
6.  auto_ptr支持所拥有的指针类型之间的隐式类型转换。
    
7.  可以通过*和->运算符对auto_ptr所有用的指针进行提领操作；
    
8.  T* get(),获得auto_ptr所拥有的指针；T* release()，释放auto_ptr的所有权，并将所有用的指针返回



## 27. shared_ptr和unique_ptr在发生异常而导致的内存泄漏的时候能否替代auto_ptr解决问题




## 28. unordered_map实现原理、底层结构
### 模板定义
```c++
template<typename _Key, typename _Tp,
	 typename _Hash = std::hash<_Key>,
	 typename _Pred = std::equal_to<_Key>,
	 typename _Alloc = std::allocator<std::pair<const _Key, _Tp> > >
class unordered_map
```

### operator[]
```c++
template<typename K, typename Pair, typename Hashtable>
    typename map_base<K, Pair, extract1st<Pair>, true, Hashtable>::mapped_type&
    map_base<K, Pair, extract1st<Pair>, true, Hashtable>::
    operator[](const K& k)
    {
      Hashtable* h = static_cast<Hashtable*>(this);
      typename Hashtable::hash_code_t code = h->m_hash_code(k);
      std::size_t n = h->bucket_index(k, code, h->bucket_count());
 
      typename Hashtable::node* p = h->m_find_node(h->m_buckets[n], k, code);
      if (!p)
		return h->m_insert_bucket(std::make_pair(k, mapped_type()),
				  n, code)->second;
      return (p->m_v).second;
    }
```

1.  计算 key 的 hash_code
    
2.  找到对应的 bucket
    
3.  在 bucket 中寻找 key 值对应的 node
    
4.  若存在对应 node, 则返回 node 对应值，若不存在，则插入一个新节点，返回新节点的值 (对应类型默认值)


当发生hash冲突时，将所有`hashcode`相同的节点都插入到同一个链表中，如下图所示。由于采用的是头部插入法，那么即便是发生了hash冲突，此时插入时间复杂度也依然是O(1)。
![[Pasted image 20230422232006.png]]
上述解决hash冲突的方法，叫做**开链法**。此时，hash冲突的问题似乎解决了，能够插入多个hashcode一样的节点，并且插入操作的时间复杂度仍然是O(1)。

但是!!!，当出现严重的hash冲突，会造成`bucket[idx]`指向的链表节点很长，此时搜索和删除一个节点的时间复杂度最坏却可能变成O(N)，即哈希表已经退化成链表，那么就违背了一开始设计hashtable的初衷，即弥补数组O(N)的搜索、删除时间复杂度。

为了解决hash退化，引入了两个概念：

-   负载因子（load_factor），是hashtable的元素个数与hashtable的桶数之间比值；
-   最大负载因子（max_load_factor），是负载因子的上限

当hashtable中的元素个数与桶数比值load_factor >= max_load_factor时，hashtable就自动发生Rehash行为，来降低load_factor：
	扩容。即使分配一块更大内存，来容纳更多的桶。
	重新插入。按照上述插入步骤将原来桶中的buck_size个节点重新插入到新的桶中。
Rehash后，桶数增加了而元素个数不变，再次满足load_factor < max_load_factor条件。


hashtable 一个很重要的影响性能的因素是冲突率，即一个桶中的元素个数。而决定冲突率的则是 hash 函数和桶的个数，所以 hash 函数的定义和 rehash 的时机格外重要.


### need_rehash
 need_rehash 函数，判断是否需要 rehash 的主要是依靠m_max_load_factor, 这是元素个数和桶的个数的比例，默认为 1.0, 若桶数目过少 hashtable 便会进行一次 rehash.

```c++
inline std::pair<bool, std::size_t>
	prime_rehash_policy::
	need_rehash(std::size_t n_bkt, std::size_t n_elt, std::size_t n_ins) const
	{
		if (n_elt + n_ins > m_next_resize)
		{
			float min_bkts = (float(n_ins) + float(n_elt)) / m_max_load_factor;
			if (min_bkts > n_bkt)
			{
				min_bkts = std::max(min_bkts, m_growth_factor * n_bkt);
				const unsigned long* const last = X<>::primes + X<>::n_primes;
				const unsigned long* p = std::lower_bound(X<>::primes, last,
									  min_bkts, lt());
				m_next_resize =
				  static_cast<std::size_t>(std::ceil(*p * m_max_load_factor));
				return std::make_pair(true, *p);
			}
			else
			{
				m_next_resize =
				  static_cast<std::size_t>(std::ceil(n_bkt * m_max_load_factor));
				return std::make_pair(false, 0);
			}
		}
		else
			return std::make_pair(false, 0);
	}
```

### rehash
接下来是 rehash 函数，rehash 其实就是新开辟一块空间，重新计算 hash 将元素放入，然后释放原来的空间
```c++
template<typename K, typename V,
		 typename A, typename Ex, typename Eq,
		 typename H1, typename H2, typename H, typename RP,
	     bool c, bool ci, bool u>
    void hashtable<K, V, A, Ex, Eq, H1, H2, H, RP, c, ci, u>::
    m_rehash(size_type n)
    {
		node** new_array = m_allocate_buckets(n);
	try
	{
		for (size_type i = 0; i < m_bucket_count; ++i)
		{
			while (node* p = m_buckets[i])
			{
				size_type new_index = this->bucket_index(p, n);
				m_buckets[i] = p->m_next;
				p->m_next = new_array[new_index];
				new_array[new_index] = p;
			}
			m_deallocate_buckets(m_buckets, m_bucket_count);
			m_bucket_count = n;
			m_buckets = new_array;
		}
		catch(...)
		{
		// A failure here means that a hash function threw an exception.
		// We can't restore the previous state without calling the hash
		// function again, so the only sensible recovery is to delete
		// everything.
			m_deallocate_nodes(new_array, n);
			m_deallocate_buckets(new_array, n);
			m_deallocate_nodes(m_buckets, m_bucket_count);
			m_element_count = 0;
			__throw_exception_again;
		}
    }
```

### hash
最后是 hash 函数，这里的 hash 函数是 std::string 的.
```c++

template<>
    struct Fnv_hash<8>
    {
		static std::size_t
		hash(const char* first, std::size_t length)
		{
			std::size_t result = static_cast<std::size_t>(14695981039346656037ULL);
			for (; length > 0; --length)
			{
				result ^= (std::size_t)*first++;
				result *= 1099511628211ULL;
			}
			return result;
		}
    };

```



## 29. unordered_map发生哈希冲突了是怎么解决的

### 1. 判断冲突位置的桶（链表）是否已满
### 2. 若满了则rehash
### 3. 若未满则开链法头插入链表



## 30. 常见解决哈希冲突的办法

### 1. 开放地址法
#### 线性探测再散列
#### 二次探测再散列
#### 伪随机数探测再散列

### 2. 拉链法
### 3. 再散列法
### 4. 建立公共溢出区


## 31. 什么是LRU？简述一下原理——get、put函数操作过程
最近最少使用 （Least recently used）是一种缓存淘汰策略。

LRU 算法实际上是让你设计数据结构：首先要接收一个 capacity 参数作为缓存的最大容量，然后实现两个 API，一个是 put(key, val) 方法存入键值对，另一个是 get(key) 方法获取 key 对应的 val，如果 key 不存在则返回 -1。

LRU 缓存算法的核心数据结构就是哈希链表，双向链表和哈希表的结合体。这个数据结构长这样：
![[Pasted image 20230422233900.jpg]]

```c++
class LRUCache {
private:
    /* 节点结构体 */
    struct Node {
        int key, val;
        Node* next;
        Node(int _key, int _val) : key(_key), val(_val), next(nullptr) {}
    };
    /* 哈希表存储节点指针 */
    unordered_map<int, Node*> map;
    /* 虚拟头节点 */
    Node* head;
    /* 虚拟尾节点 */
    Node* tail;
    /* 缓存容量 */
    int capacity;

public:
    LRUCache(int _capacity) : capacity(_capacity) {
        /* 初始化虚拟头尾节点 */
        head = new Node(-1, -1);
        tail = new Node(-1, -1);
        head->next = tail;
        tail->next = nullptr;
    }

    int get(int key) {
        if (!map.count(key)) {
            return -1;
        }
        /* 通过哈希表定位到节点 */
        Node* node = map[key];
        /* 将节点移动到链表尾部 */
        moveToEnd(node);
        return node->val;
    }

    void put(int key, int value) {
        if (map.count(key)) {
            /* 如果已存在，修改对应节点的值，并移动到链表尾部 */
            Node* node = map[key];
            node->val = value;
            moveToEnd(node);
            return;
        }
        if (map.size() == capacity) {
            /* 如果超出容量，删除链表头部节点 */
            deleteHead();
        }
        /* 添加新的节点到链表尾部，并在哈希表中添加映射 */
        addNode(new Node(key, value));
    }

private:
    /* 将节点移动到链表尾部 */
    void moveToEnd(Node* node) {
        removeNode(node);
        addNode(node);
    }

    /* 删除链表头部节点 */
    void deleteHead() {
        Node* node = head->next;
        removeNode(node);
        map.erase(node->key);
        delete node;
    }

    /* 添加节点到链表尾部 */
    void addNode(Node* node) {
        tail->next = node;
        node->next = nullptr;
        map[node->key] = node;
        tail = node;
    }

    /* 删除链表中的节点 */
    void removeNode(Node* node) {
        Node* prev = head;
        while (prev->next != node) {
            prev = prev->next;
        }
        prev->next = node->next;
    }
};

```

`put` 方法逻辑图：
![[Pasted image 20230422234146.jpg]]



## 32. MySQL数据库中锁的分类，详细说一下。

根据加锁的范围，可以分为**全局锁、表级锁和行锁**三类
![[1e37f6994ef44714aba03b8046b1ace2.webp]]
[[MySQL笔记#五、锁]]


## 33. 乐观锁和悲观锁

### 悲观锁
悲观锁做事⽐较悲观，它认为多线程同时修改共享资源的概率⽐较⾼，于是很容易出现冲突，所以访问共享资源前，先要上锁。==互斥锁、⾃旋锁、读写锁，都是属于悲观锁==

### 乐观锁
那相反的，如果多线程同时修改共享资源的概率⽐较低，就可以采⽤乐观锁。
乐观锁做事⽐较乐观，它假定冲突的概率很低，它的⼯作⽅式是：先修改完共享资源，再验证这段时间内有没有发⽣冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。

放弃后如何重试，这跟业务场景息息相关，虽然重试的成本很⾼，但是冲突的概率⾜够低的话，还是可以接受的。

可⻅，乐观锁的⼼态是，不管三七⼆⼗⼀，先改了资源再说。另外，你会发现乐观锁全程并没有加锁，所以它也叫⽆锁编程。
常⻅的 ==SVN 和 Git 也是⽤了乐观锁==的思想，先让⽤户编辑代码，然后提交的时候，通过版本号来判断是否产⽣了冲突，发⽣了冲突的地⽅，需要我们⾃⼰修改后，再重新提交。


## 34. 跳表是什么；或者说如何优化一个数据量很大的链表的搜索过程。

跳表-一级索引

![[19063731-4f4535e6d0959c32.webp]]

跳錶二級索引
![[19063731-3852cc36af701f46.webp]]

假如一直往原始列表中添加数据，但是不更新索引，就可能出现两个索引节点之间数据非常多的情况，极端情况，跳表退化为单链表，从而使得查找效率从 O(logn) 退化为 O(n)。
### 索引更新
#### 1、完全重建索引
每次插入数据后，都把这个跳表的索引删掉全部重建，重建索引的时间复杂度是多少呢？因为索引的空间复杂度是 O(n)，即：索引节点的个数是 O(n) 级别，每次完全重新建一个 O(n) 级别的索引，时间复杂度也是 O(n) 。造成的后果是：为了维护索引，导致每次插入数据的时间复杂度变成了 O(n)。

#### 2、按照概率隨機選擇索引 
**随机选 n/2 个元素做为一级索引、随机选 n/4 个元素做为二级索引、随机选 n/8 个元素做为三级索引，依次类推，一直到最顶层索引**。

在每次新插入元素的时候，尽量让该元素有 1/2 的几率建立一级索引、1/4 的几率建立二级索引、1/8 的几率建立三级索引，以此类推，就能满足我们上面的条件。现在我们就需要一个概率算法帮我们把控这个 1/2、1/4、1/8 ... ，**当每次有数据要插入时，先通过概率算法告诉我们这个元素需要插入到几级索引中**，然后开始维护索引并把数据插入到原始链表中。

实现一个 randomLevel() 方法，该方法会随机生成 1~MAX_LEVEL 之间的数（MAX_LEVEL表示索引的最高层数），且该方法**有 1/2 的概率返回 1、1/4 的概率返回 2、1/8的概率返回 3，以此类推**。
```cpp
// 该 randomLevel 方法会随机生成 1~MAX_LEVEL 之间的数，且 ：
//        1/2 的概率返回 1
//        1/4 的概率返回 2
//        1/8 的概率返回 3 以此类推
private int randomLevel() {
  int level = 1;
  // 当 level < MAX_LEVEL，且随机数小于设定的晋升概率时，level + 1
  while (Math.random() < SKIPLIST_P && level < MAX_LEVEL)
    level += 1;
  return level;
}
```

![[19063731-1a7f35e43819c9c4.webp]]


跳表删除数据时，要把索引中对应节点也要删掉。如下图所示，如果要删除元素 9，需要把原始链表中的 9 和第一级索引的 9 都删除掉。
![[19063731-e95c396e6e62bc87.webp]]

插入数据时维护索引的时间复杂度是多少呢？**元素插入到单链表的时间复杂度为 O(1)**，我们索引的高度最多为 logn，当插入一个元素 x 时，最坏的情况就是元素 x 需要插入到每层索引中，所以插入数据到各层索引中，最坏时间复杂度是 O(logn)。

删除元素的过程跟查找元素的过程类似，只不过在查找的路径上如果发现了要删除的元素 x，则执行删除操作。跳表中，每一层索引其实都是一个有序的单链表，单链表删除元素的时间复杂度为 O(1)，索引层数为 logn 表示最多需要删除 logn 个元素，所以删除元素的总时间包含 _查找元素的时间_ 加 _删除 logn个元素的时间_ 为 O(logn) + O(logn) = 2 O(logn)，忽略常数部分，删除元素的时间复杂度为 O(logn)。


Redis选择使用跳表实现有序集合


## 35. Innodb引擎的底层数据结构是什么，（B+树数据结构
[[MySQL笔记#3、数据页与B+树]]
B+树数据结构
![[3104c8c3adf36e8931862fe8a0520f5d.webp]]


 单个数据页的结构
![[261011d237bec993821aa198b97ae8ce.webp]]

## 36. vector,list,map,unordered_map的区别，底层结构

1.vector 底层数据结构为数组 ，支持快速随机访问

2.list 底层数据结构为双向链表，支持快速增删

3.deque 底层数据结构为一个中央控制器和多个缓冲区，详细见STL源码剖析P146，支持首尾（中间不能）快速增删，也支持随机访问

deque是一个双端队列(double-ended queue)，也是在堆中保存内容的.它的保存形式如下:

[堆1] --> [堆2] -->[堆3] --> ...

每个堆保存好几个元素,然后堆和堆之间有指针指向,看起来像是list和vector的结合品.

4.stack 底层一般用list或deque实现，封闭头部即可，不用vector的原因应该是容量大小有限制，扩容耗时

5.queue 底层一般用list或deque实现，封闭头部即可，不用vector的原因应该是容量大小有限制，扩容耗时（stack和queue其实是适配器,而不叫容器，因为是对容器的再封装）

6.priority_queue 的底层数据结构一般为vector为底层容器，堆heap为处理规则来管理底层容器实现

7.set 底层数据结构为红黑树，有序，不重复

8.multiset 底层数据结构为红黑树，有序，可重复

9.map 底层数据结构为红黑树，有序，不重复

10.multimap 底层数据结构为红黑树，有序，可重复

11.unordered_set 底层数据结构为hash表，无序，不重复

12.unordered_multiset 底层数据结构为hash表，无序，可重复

13.unordered_map 底层数据结构为hash表，无序，不重复

14.unordered_multimap 底层数据结构为hash表，无序，可重复



### vector
vector底层是一个动态数组，包含三个迭代器，start和finish之间是已经被使用的空间范围，end_of_storage是整块连续空间包括备用空间的尾部。

当空间不够装下数据（vec.push_back(val)）时，会自动申请另一片更大的空间（1.5倍或者2倍），然后把原来的数据拷贝到新的内存空间，接着释放原来的那片空间【vector内存增长机制】。若原大小为0，则扩充为1；若原大小不为0，则扩充为两倍。


当释放或者删除（vec.clear()）里面的数据时，其存储空间不释放，仅仅是清空了里面的数据。

vec.clear()：清空内容，但是不释放内存。
k
vector().swap(vec)：清空内容，且释放内存，想得到一个全新的vector。

vec.shrink_to_fit()：请求容器降低其capacity和size匹配。

vec.clear();vec.shrink_to_fit();：清空内容，且释放内存。

### list
List和Vector都是STL的顺序容器，==唯一不同的地方就在于：Vector是一段连续的内存空间，List则是一段不连续的内存空间==，相比于Vector来说，**List在每次插入和删除的时候，只需要配置或释放一个元素空间**，对于任何位置的插入和删除操作，List永远能做到常数时间。但是，List由于不连续的内存空间，导致不支持随机寻址，所以尺有所长寸有所短，在程序中选择使用那种容器还要视元素的构造复杂度和存取行为而定。

List就是一个双向链表

### map
map的特性是所有元素会根据键值进行自动排序。map中所有的元素都是pair，拥有键值(key)和实值(value)两个部分，并且不允许元素有相同的key
一旦map的key确定了，那么是无法修改的，但是可以修改这个key对应的value，因此map的迭代器既不是constant iterator，也不是mutable iterator
标准STL map的底层机制是RB-tree（红黑树），另一种以hash table为底层机制实现的称为hash_map。


## 37. 又问了unordered_map哈希冲突的解决方式和常见解决哈希冲突方法



## 38. 栈和堆的区别
[[C++基础语法#2. 堆和栈]] 

堆向上，向高地址方向增长。 栈向下，向低地址方向增长。
- 栈顶和栈底是之前预设好的，栈是向栈底扩展，大小固定
- 堆向高地址扩展，是不连续的内存区域

堆和栈的区别 申请方式不同。 栈由系统自动分配。 堆是自己申请和释放的。 



## 39. new和malloc的区别
[[C++基础语法#3. new、delete、malloc、free]]

- 相同点 都可用于内存的动态申请和释放 
- 不同点 
- 前者是C++运算符，后者是C/C++语言标准库函数 
- new自动计算要分配的空间大小，malloc需要手工计算 new是类型安全的，malloc不是。



## 40. 面向对象的三大特性？具体点说
[[C++基础语法#三大特性继承封装多态]]
### 封装

数据和代码捆绑在一起，避免外界干扰和不确定性访问。 封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏，

### 继承

### 多态

同一事物表现出不同事物的能力，即向不同对象发送同一消息，不同的对象在接收时会产生不同的行为（重载实现编译时多态，虚函数实现运行时多态）**。 多态性是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单一句话：允许将子类类型的指针赋值给父类类型的指针

41. 空类的大小？加了个 int 呢？为什么？
42. 说一说 C++ 的 static
43. 说一说 inline
44. 多态实现的原理？
45. 构造函数和析构函数可以是虚函数吗？
## 46. 说一说 C++ STL 容器
### 两级空间配置器
我们知道动态开辟内存时，要在堆上申请，但若是我们需要

频繁的在堆开辟释放内存，则就会**在堆上造成很多外部碎片**，浪费了内存空间；

每次都要进行调用**malloc、free**函数等操作，使空间就会增加一些附加信息，降低了空间利用率；

随着外部碎片增多，内存分配器在找不到合适内存情况下需要合并空闲块，浪费了时间，大大降低了效率。

于是就设置了二级空间配置器，**当开辟内存<=128bytes时，即视为开辟小块内存，则调用二级空间配置器。**

关于STL中一级空间配置器和二级空间配置器的选择上，一般默认**选择的为二级空间配置器**。 如果大于128字节再转去一级配置器器。

![[Pasted image 20230807163114.png]]
维护16条链表，分别是0-15号链表，最小8字节，以8字节逐渐递增，最大128字节，你传入一个字节参数，表示你需要多大的内存，会自动帮你校对到第几号链表（如需要13bytes空间，我们会给它分配16bytes大小），在找到第n个链表后查看链表是否为空，如果不为空直接从对应的free_list中拔出，将已经拨出的指针向后移动一位。

### 容器
#### vector
vector数据结构 vector和数组类似，拥有一段连续的内存空间，并且起始地址不变。因此能高效的进行随机存取，时间复杂度为o(1);
但因为内存空间是连续的，所以在进行插入和删除操作时，会造成内存块的拷贝，时间复杂度为o(n)。


- vector中的迭代器在使用后就失效了，而list的迭代器在使用之后还可以继续使用。

size()函数返回的是已用空间大小，
capacity()返回的是总空间大小，
capacity()-size()则是剩余的可用空间大小。
当size()和capacity()相等，说明vector目前的空间已被用完，如果再添加新元素，则会引起vector空间的动态增长。

使用reserve(n)预先分配一块较大的指定大小的内存空间，这样当指定大小的内存空间未使用完时，是不会重新分配内存空间的，这样便提升了效率。只有当n>capacity()时，调用reserve(n)才会改变vector容量。


1、空的vector对象，size()和capacity()都为0

2、当空间大小不足时，新分配的空间大小为原空间大小的2倍。
（不同的编译器，vector有不同的扩容大小。在vs下是1.5倍，在GCC下是2倍）

==采用成倍方式扩容，可以保证常数的时间复杂度==，而增加指定大小的容量只能达到O(n)的时间复杂度，因此，使用成倍的方式扩容。

（使用k=2增长因子的问题在于，每次扩展的新尺寸必然刚好大于之前分配的总和，也就是说，之前分配的内存空间不可能被使用。这样对内存不友好，最好把增长因子设为(1, 2)，也就是1-2之间的某个数值。）

3、使用reserve()预先分配一块内存后，在空间未满的情况下，不会引起重新分配，从而提升了效率。

4、当reserve()分配的空间比原空间小时，是不会引起重新分配的。

5、resize()函数只改变容器的元素数目，未改变容器大小。

6、用reserve(size_type)只是扩大capacity值，这些内存空间可能还是“野”的，如果此时使用“[ ]”来访问，则可能会越界。而resize(size_type new_size)会真正使容器具有new_size个对象。

所有内存空间是在vector析构时候才能被系统回收。empty()用来检测容器是否为空的，clear()可以清空所有元素。但是即使clear()，vector所占用的内存空间依然如故，无法保证内存的回收。

如果使用vector，可以用swap()来帮助你释放多余内存或者清空全部内存。
```c++
vector(Vec).swap(Vec); //将Vec中多余内存清除； 
vector().swap(Vec); //清空Vec的全部内存；
```

#### deque
vector是单向开口（尾部）的连续线性空间，deque则是一种双向开口的连续线性空间，虽然vector也可以在头尾进行元素操作，但是其头部操作的效率十分低下（主要是涉及到整体的移动）
![[Pasted image 20230807165656.png]]

deque和vector的最大差异一个是deque运行在常数时间内对头端进行元素操作，
二是deque没有容量的概念，它是动态地以分段连续空间组合而成，可以随时增加一段新的空间并链接起来

deque虽然也提供随机访问的迭代器，但是其迭代器并不是普通的指针，其复杂程度比vector高很多，因此除非必要，否则一般使用vector而非deque。如果需要对deque排序，可以先将deque中的元素复制到vector中，利用sort对vector排序，再将结果复制回deque

deque由一段一段的定量连续空间组成，一旦需要增加新的空间，只要配置一段定量连续空间拼接在头部或尾部即可，因此deque的最大任务是如何维护这个整体的连续性

#### list
list数据结构 list是由双向链表实现的，因此内存空间是不连续的。只能通过指针访问数据，所以list的随机存取非常没有效率，时间复杂度为o(n);但由于链表的特点，能高效地进行插入和删除。非连续存储结构：list是一个双链表结构，支持对链表的双向遍历。每个节点包括三个信息：元素本身，指向前一个元素的节点（prev）和指向下一个元素的节点（next）。因此list可以高效率的对数据元素任意位置进行访问和插入删除等操作。由于涉及对额外指针的维护，所以开销比较大。

list不仅是一个双向链表，而且还是一个环状双向链表，所以它只需要一个指针；
    
list不像vector那样有可能在空间不足时做重新配置、数据移动的操作，所以插入前的所有迭代器在插入操作之后都仍然有效；

deque是一种双向开口的连续线性空间，所谓双向开口，意思是可以在头尾两端分别做元素的插入和删除操作；

#### map set
他们的底层都是以红黑树的结构实现，因此插入删除等操作都在O(logn时间内完成，因此可以完成高效的插入删除；

红黑树的特性：

- 每个节点不是红色就是黑色
- 根结点为黑色
- 如果节点为红色，其子节点必为黑（黑节点子节点可为黑）。
- 任一节点至（NULL）树尾端的任何路径，所含的黑节点数量必相同

- 它是二叉排序树（继承二叉排序树特性）：
- 若左子树不空，则左子树上所有结点的值均小于或等于它的根结点的值。
- 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值。
    - 左、右子树也分别为二叉排序树
- 查找时间一定可以控制在O(logn)。


## [#](https://interviewguide.cn/notes/03-hunting_job/02-interview/01-04-02-STL.html#_37%E3%80%81stl%E4%B8%ADunordered-map%E5%92%8Cmap%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF)37、STL中unordered_map和

因为map和set要求是自动排序的，红黑树能够实现这一功能，而且时间复杂度比较低。

### 配接器

#### stack
stack（栈）是一种先进后出（First In Last Out）的数据结构，只有一个入口和出口，那就是栈顶，除了获取栈顶元素外，没有其他方法可以获取到内部的其他元素，其结构图如下：
![[Pasted image 20230807170027.png]]
stack这种“修改某种接口，形成另一种风貌”的行为，成为adapter(配接器)。常将其归类为container adapter而非container
stack除了默认使用deque作为其底层容器之外，也可以使用双向开口的list，只需要在初始化stack时，将list作为第二个参数即可。由于stack只能操作顶端的元素，因此其内部元素无法被访问，也不提供迭代器。

#### queue
queue（队列）是一种先进先出（First In First Out）的数据结构，只有一个入口和一个出口，分别位于最底端和最顶端，出口元素外，没有其他方法可以获取到内部的其他元素




## map插入方式有哪几种？

1. 用insert函数插入pair数据，
```
mapStudent.insert(pair<int, string>(1, "student_one")); 
```
2. 用insert函数插入value_type数据
```
mapStudent.insert(map<int, string>::value_type (1, "student_one"));
```
3. 在insert函数中使用make_pair()函数
```
mapStudent.insert(make_pair(1, "student_one")); 
```
4. 用数组方式插入数据
```
mapStudent[1] = "student_one";
```

unordered_map和map类似，都是存储的key-value的值，可以通过key快速索引到value。不同的是unordered_map不会根据key的大小进行排序
unordered_map的底层实现是hash_table; hash_table使用开链法解决冲突

##### map中[]与find的区别？
map的下标运算符[]的作用是：将关键码作为下标去执行查找，并返回对应的值；如果不存在这个关键码，就将一个具有该关键码和值类型的默认值的项插入这个map。

map的find函数：用关键码执行查找，找到了返回该位置的迭代器；如果不存在这个关键码，就返回尾迭代器。

erase()函数，只能删除内容，不能改变容量大小;


1. 说一说智能指针
2. 说一说进程和线程
3. 线程间的同步手段？用过哪些
4. TCP 的三次握手和四次挥手
5. 讲讲了解到c++11新特性。
6. std::string缺陷（不了解）。
7. 智能指针与原生指针开销一样吗？（答智能指针是一个类，有默认的拷贝、构造函数等，shared_ptr还多了一个引用计数），说这是基于类层面的，让我基于对象回答。不是很了解再次询问，说单看10000个智能指针和原生指针开销一样吗？有点懵了说一样。
8.  浮点数存储方式。
9.  MD5，Base64。
10.  http1.0/1 2 3做了哪些优化。
11.  utf8 utf16区别（不知道）， c++用的哪个。
12.  cookie。
13.  服务器项目实现了哪些功能，get post两者语义上有啥区别，如何保证安全性。你的服务器如何保证安全（答参考https）。
14.  https如何保证安全（tsl握手），追问如何握手（凭记忆瞎掰了一下。）。
15.  sql注入，危害， 如何解决。
16.  市面上出现了很多信息泄露的安全问题如何解决（好像是这个意思）。不知道。
17.  \r 和 \n有什么区别。
18.  讲讲虚函数如何实现动态分发？每个类都有虚函数表吗？
19. 说一下http的get和post的区别  
20. 解释一下安全和幂等  
21. TCP连接和断开的过程  
22. 进程通信有哪些方法，它们的特点  
23. 说一下进程、线程、协程  
24. 介绍一下IO多路复用，和之前的方法相比有什么优点  
25. syn攻击有了解么  
26. mysql学过哪些知识
27. 操作系统里进程的调度方法？响应比的概念？
28. windows中内存的储存方式
29. linux查询文件权限的命令
30. osi模型？
31. http、tcp、udp是哪一层的
32. tcp、udp的应用场景
33. 了解的排序算法？快排最差情况复杂度
34. 一致性哈希？哈希冲突的解决方式
35. mysql中ACID四大特性
36. dp和分治算法的区别和应用场景
37. C++引用和指针的区别
38. C++内存怎么储存
39. C++对象只分配在堆和栈上怎么操作
40. tcp三次握手
41. sql注入
42. C++设计模式
43. 红黑树/二叉搜索树B+树有了解吗
44. TCP拥塞控制
45. 在项目中的哪里用到进程线程，怎么考虑的
46. C/C++有什么区别
47. 什么是虚表
48. C++的继承有没有什么缺点
49. C++和Golang的指针有什么区别
50. 开发一个存储引擎，你会选择C++还是Golang，为什么
51. 你开发的功能的过程中，你会关注哪些方面
52. 你在项目中是怎么排查bug并且解决的，可以介绍一下吗
53. C++内存主要有哪些区，讲讲
54. vector实现和扩容
55. vector earse，迭代器失效
56. c++编译过程
57. 析构为什么要声明为virtual。经典八股
58. 如何避免隐式转换。explicit
59. 如何禁用拷贝和赋值。私有、delete、uncopyable基类
60. 如何理解多态
61. 虚函数和纯虚函数的区别
62. 为什么要有纯虚函数（虚函数不就够了吗）
63. 构造和析构的调用顺序（送分）
64. C++ 内存分配了解吗
65. new 和 malloc 的区别和实现原理
66. 还有哪些内存区（除了堆和栈）
67. 静态变量和全局变量存放在哪个区
68. new 和 malloc 分配的内存在哪个区
69. vector 底层实现
70. 你平时用 vector 做什么
71. 说一下 vector 扩容原理
72. vector 在哪个区分配内存
73. 说一下你了解的各种 map
74. map 和 unordered_map 的底层实现和各自的优势（插入、删除、查找谁更快）
75. 你平时用 unordered_map 做什么
76. 介绍一下 unique_ptr 和 shared_ptr
77. 你了解 lambda 表达式吗
78. 了解函数指针吗（std::function）
79. 函数模板了解多少
80. 函数模板内的参数什么阶段会被替换掉
81. 说一下线程池的实现步骤
82. 还有哪些线程同步的方法（除了锁和条件变量）
83. c++为什么要虚析构？  
84. 为什么不虚构造？  
85. 说说快排和冒泡
86. c++三大特性?  
87. 说说多态  
88. 说说虚函数  
89. 说说c++11新特性  
90. 讲讲智能指针  
91. 讲讲右值引用  
92. 讲一下多线程  
93. linux了解吗？  
94. 说说常用指令  
95. 网络出现故障一般用什么指令排查？  
96. 讲一下TCP如何建立链接？
97. 说一下 C++ 程序从编译到运行的过程是什么
98. 预编译这个过程是做什么的
99. 动态链接和静态链接的区别
100. 宏定义和内联函数的区别
101. 什么情况下用内联，什么时候用宏定义
102. C 里面有 malloc 了，为什么 C++ 还要引入 new
103. 有没有听过内存池，讲一下内存池
104. 展开说一下 STL 中的内存池
105. 池化的技术除了内存池外，你还听过哪些
106. 连接池你说说你是怎么理解的
107. 什么是内存泄漏
108. 如果服务器上出现了内存泄漏，应该怎么排查
109. 内存泄漏可能会带来什么风险
110. 面向对象的三大特性是什么，分别具体讲一下它们的含义
111. 多继承有没有听过菱形继承，讲一讲它可能会出现的问题
112. 菱形继承出现的问题该怎么解决
113. 什么是内存对齐
114. 你说一下虚函数的模型是怎么样的
115. 如果将一个成员声明为 static，它与普通成员变量有什么区别
116. 如果一个类有很多子类，那么这个类中的方法是被子类共享的还是一人一个副本
117. 举个例子说一下
118. 说一下你对 this 指针的理解
119. 你介绍一下异常处理是什么
120. 异常有哪些
121. 编译时异常和运行时异常有什么区别
122. STL 中对比一下 unordered_map 和 map 的区别
123. 当 unordered_map 中元素比较多的时候，一个链表上元素可能会比较多，这28. 时候查询就比较慢该怎么办
124. 说一下优先队列可以用来做什么
125. 智能指针你都了解哪些，具体说一下它们的作用
126. 说一下匿名函数
127. lambda 表达式的优势是什么
128. 说一下移动构造函数和拷贝构造函数的区别
129. 解释一下右值引用是什么，它与左值引用的区别
130. 在写多线程代码时，有什么方式可以保证同步
131. template 模版你是怎么理解的
132. 模版的底层是怎么实现的 —— 这个地方确实忘了T_T
133. 纯虚函数是什么